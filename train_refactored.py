# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€è®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ
æ”¯æŒ chsims/chsimsv2/meld æ•°æ®é›†
"""

import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from typing import Dict

from config_refactored import get_config
from data_loader_refactored import create_dataloaders_refactored
from smsa_refactored import MultimodalEmotionModel_Refactored
from utils import (
    setup_seed, setup_logger, dict_to_str,
    MetricsCalculator, EarlyStopping,
    save_checkpoint, count_parameters,
    save_config, AverageMeter,
    MetricsHistory, TrainingPlotter
)

# å°è¯•å¯¼å…¥æ¨¡æ€åˆ†æå™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from modality_contribution_analyzer import ModalityContributionAnalyzer
    MODALITY_ANALYZER_AVAILABLE = True
except ImportError:
    MODALITY_ANALYZER_AVAILABLE = False


class Trainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = config.device
        
        # è®¾ç½®éšæœºç§å­
        setup_seed(config.seed)
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(config.log_dir, 'train')
        self.logger.info(str(config))
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(config.save_dir, exist_ok=True)
        
        # è®¾ç½®æŒ‡æ ‡è®°å½•æ–‡ä»¶
        self.metrics_file = getattr(config, 'metrics_file', None)
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.setup_data()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.setup_model()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.setup_optimizer()
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.setup_criterion()
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨å’Œæ—©åœ
        self.metrics_calc = MetricsCalculator()
        self.early_stopping = EarlyStopping(
            patience=config.early_stop_patience,
            mode=config.metric_mode,
            verbose=True,
        )
        
        self.best_metric = float('inf') if config.metric_mode == 'min' else float('-inf')
        self.best_epoch = 0
        
        # ====== æ¨¡æ€è´¡çŒ®åº¦åˆ†æå™¨ ======
        enable_analysis = getattr(config, 'enable_modality_analysis', False)
        if MODALITY_ANALYZER_AVAILABLE and enable_analysis:
            self.modality_analyzer = ModalityContributionAnalyzer(modalities=['social', 'context'])
            self.analyze_modality_every = getattr(config, 'analyze_modality_every', 10)
            self.modality_analysis_enabled = True
            self.modality_analysis_epochs = getattr(config, 'modality_analysis_epochs', 3)
            self.logger.info(f"âœ“ æ¨¡æ€åˆ†æå™¨å·²å¯ç”¨ (æ¯{self.analyze_modality_every}ä¸ªbatchåˆ†æ)")
            self.logger.info(f"  âš ï¸  æ³¨æ„ï¼šå³æ—¶æ¶ˆèåˆ†æä¼šå½±å“è®­ç»ƒé€Ÿåº¦")
        else:
            self.modality_analyzer = None
            self.modality_analysis_enabled = False
            if not enable_analysis:
                self.logger.info("â„¹ï¸  æ¨¡æ€åˆ†æå·²ç¦ç”¨ï¼ˆå¯é€šè¿‡ --enable_modality_analysis å¯ç”¨ï¼‰")
            elif not MODALITY_ANALYZER_AVAILABLE:
                self.logger.info("âš ï¸  æ¨¡æ€åˆ†æå™¨æœªå®‰è£…ï¼ˆmodality_contribution_analyzer.pyï¼‰")
        # ==========================================
        
        # ====== è®­ç»ƒæ›²çº¿ç»˜å›¾ ======
        self.metrics_history = MetricsHistory()
        
        # ç»˜å›¾é…ç½®
        self.plot_config = {
            'mae': getattr(config, 'plot_mae', False),
            'acc_2': getattr(config, 'plot_acc2', False),
            'acc_3': getattr(config, 'plot_acc3', False),
            'acc_5': getattr(config, 'plot_acc5', False),
            'loss': getattr(config, 'plot_loss', False),
            'corr': getattr(config, 'plot_corr', False),
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç»˜å›¾å¼€å…³æ‰“å¼€
        self.plotting_enabled = any(self.plot_config.values())
        
        if self.plotting_enabled:
            self.plotter = TrainingPlotter(config.save_dir, self.logger)
            enabled_plots = [k for k, v in self.plot_config.items() if v]
            self.logger.info(f"âœ“ è®­ç»ƒæ›²çº¿ç»˜å›¾å·²å¯ç”¨: {', '.join(enabled_plots)}")
        else:
            self.plotter = None
            self.logger.info("â„¹ï¸  è®­ç»ƒæ›²çº¿ç»˜å›¾å·²ç¦ç”¨ï¼ˆå¯é€šè¿‡ --plot_mae ç­‰å¼€å…³å¯ç”¨ï¼‰")
        # ==========================================
        
        # ====== è¯¾ç¨‹å­¦ä¹ é…ç½® ======
        self.curriculum_mode = getattr(config, 'curriculum_mode', 'none')
        self.curriculum_epochs = getattr(config, 'curriculum_epochs', 5)
        if self.curriculum_mode != 'none':
            self.logger.info(f"âœ“ è¯¾ç¨‹å­¦ä¹ å·²å¯ç”¨: mode={self.curriculum_mode}, epochs={self.curriculum_epochs}")
        else:
            self.logger.info("â„¹ï¸  è¯¾ç¨‹å­¦ä¹ å·²ç¦ç”¨ï¼ˆå¯é€šè¿‡ --curriculum_mode å¯ç”¨ï¼‰")
        # ==========================================
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        self.logger.info("Loading data...")
        self.logger.info(f"Dataset: {self.config.dataset_name}")
        
        self.train_loader, self.valid_loader, self.test_loader = create_dataloaders_refactored(
            data_dir=self.config.data_dir,
            batch_size=self.config.batch_size,
            num_workers=self.config.num_workers,
            seq_length=self.config.seq_length,
            augment_train=self.config.augment_train,
            noise_scale=self.config.noise_scale,
            cache_size=self.config.cache_size,
        )
        
        self.logger.info(f"Train samples: {len(self.train_loader.dataset)}")
        self.logger.info(f"Valid samples: {len(self.valid_loader.dataset)}")
        self.logger.info(f"Test samples: {len(self.test_loader.dataset)}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        self.logger.info("Initializing model...")
        
        self.model = MultimodalEmotionModel_Refactored(
            text_input_dim=self.config.text_input_dim,
            audio_input_dim=self.config.audio_input_dim,
            video_input_dim=self.config.video_input_dim,
            text_global_dim=self.config.text_global_dim,
            social_dim=self.config.social_dim,
            context_dim=self.config.context_dim,
            hidden_dim=self.config.hidden_dim,
            model_config=self.config.model_config,
            num_ism_layers=self.config.num_ism_layers,
            num_coupled_layers=self.config.num_coupled_layers,
            num_labels=self.config.num_labels,
            fusion_hidden_dim=self.config.fusion_hidden_dim,
            dropout_p=self.config.dropout_p,
        ).to(self.device)
        
        num_params = count_parameters(self.model)
        self.logger.info(f"Model parameters: {num_params:,}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
        )
        
        # è®¡ç®— warmup æ­¥æ•°
        warmup_ratio = getattr(self.config, 'warmup_ratio', 0.0)
        warmup_epochs = int(self.config.num_epochs * warmup_ratio)
        
        if self.config.scheduler_type == 'step':
            base_scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.scheduler_step_size,
                gamma=self.config.scheduler_gamma,
            )
        elif self.config.scheduler_type == 'cosine':
            base_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.num_epochs - warmup_epochs,  # ä½™å¼¦é€€ç«é˜¶æ®µçš„æ€»é•¿åº¦
            )
        elif self.config.scheduler_type == 'reduce_on_plateau':
            # ReduceLROnPlateau ä¸æ”¯æŒ SequentialLRï¼Œå•ç‹¬å¤„ç†
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode=self.config.metric_mode,
                factor=self.config.scheduler_gamma,
                patience=self.config.scheduler_patience,
            )
            if warmup_epochs > 0:
                self.logger.warning(f"âš  ReduceLROnPlateau ä¸æ”¯æŒ Warmupï¼Œå·²å¿½ç•¥ warmup_ratio={warmup_ratio}")
            return
        else:
            self.scheduler = None
            return
        
        # å¦‚æœå¯ç”¨äº† warmupï¼Œä½¿ç”¨ SequentialLR ç»„åˆ warmup + ä¸»è°ƒåº¦å™¨
        if warmup_epochs > 0:
            warmup_scheduler = optim.lr_scheduler.LinearLR(
                self.optimizer,
                start_factor=0.1,  # ä» 10% å­¦ä¹ ç‡å¼€å§‹
                end_factor=1.0,    # é¢„çƒ­åˆ° 100% å­¦ä¹ ç‡
                total_iters=warmup_epochs,
            )
            self.scheduler = optim.lr_scheduler.SequentialLR(
                self.optimizer,
                schedulers=[warmup_scheduler, base_scheduler],
                milestones=[warmup_epochs],
            )
            self.logger.info(f"âœ“ å¯ç”¨å­¦ä¹ ç‡é¢„çƒ­: {warmup_epochs} epochs (warmup_ratio={warmup_ratio})")
        else:
            self.scheduler = base_scheduler
    
    def setup_criterion(self):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        if self.config.task_type == 'regression':
            # å°è¯•ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
            loss_type = getattr(self.config, 'loss_function', 'mse')
            
            if loss_type == 'l1' or loss_type == 'mae':
                self.criterion = nn.L1Loss()
                self.logger.info("âœ“ ä½¿ç”¨ L1 Loss (MAE)")
            elif loss_type == 'focal_mse':
                try:
                    from losses import FocalMSELoss
                    self.criterion = FocalMSELoss(gamma=2.0)
                    self.logger.info("âœ“ ä½¿ç”¨ Focal MSE Loss (gamma=2.0)")
                except ImportError:
                    self.logger.warning("âš ï¸  losses.pyä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†MSE Loss")
                    self.criterion = nn.MSELoss()
            elif loss_type == 'huber':
                try:
                    from losses import HuberLoss
                    self.criterion = HuberLoss(delta=1.0)
                    self.logger.info("âœ“ ä½¿ç”¨ Huber Loss (delta=1.0)")
                except ImportError:
                    self.logger.warning("âš ï¸  losses.pyä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†MSE Loss")
                    self.criterion = nn.MSELoss()
            else:
                self.criterion = nn.MSELoss()
                self.logger.info("ä½¿ç”¨æ ‡å‡† MSE Loss")
        else:
            self.criterion = nn.CrossEntropyLoss()
    
    def _apply_curriculum(self, epoch: int) -> None:
        """
        åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥
        
        ç­–ç•¥ A: freeze_backbone
            - åœ¨è¯¾ç¨‹å­¦ä¹ æœŸé—´å†»ç»“ Mamba Backboneï¼Œåªè®­ç»ƒ MoE/FiLM/Head
            - è¯¾ç¨‹ç»“æŸåè§£å†»æ‰€æœ‰å‚æ•°
        
        ç­–ç•¥ B: alpha_blending
            - æ¸è¿›å¼å¢åŠ  MoE çš„å½±å“åŠ›
            - alpha = min(1.0, (epoch + 1) / curriculum_epochs)
            - æ‰€æœ‰å‚æ•°å§‹ç»ˆå¯è®­ç»ƒ
        
        Args:
            epoch: å½“å‰ epoch ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
        """
        if self.curriculum_mode == 'none':
            return
        
        curriculum_epochs = self.curriculum_epochs
        
        if self.curriculum_mode == 'freeze_backbone':
            # ====== ç­–ç•¥ A: å†»ç»“éª¨å¹²ç½‘ç»œ ======
            if epoch <= curriculum_epochs:
                # å†»ç»“é˜¶æ®µï¼šåªè®­ç»ƒ MoE/FiLM æ¨¡å—å’Œåˆ†ç±»å¤´
                frozen_count = 0
                trainable_count = 0
                
                for name, param in self.model.named_parameters():
                    # åˆ¤æ–­æ˜¯å¦æ˜¯ MoE/FiLM/Head ç›¸å…³çš„å‚æ•°
                    is_moe_film = any(key in name for key in [
                        'social_film', 'context_film',  # MoE-FiLM æ¨¡å—
                        'classifier', 'pre_classifier',  # åˆ†ç±»å¤´
                        'modality_fusion',  # èåˆå±‚
                        'freq_fusion', 'freq_decomp',  # é¢‘åŸŸåˆ†è§£
                    ])
                    
                    if is_moe_film:
                        param.requires_grad = True
                        trainable_count += 1
                    else:
                        param.requires_grad = False
                        frozen_count += 1
                
                self.logger.info(f"ğŸ“š Curriculum: Freezing Backbone (Epoch {epoch}/{curriculum_epochs})")
                self.logger.info(f"   Frozen params: {frozen_count}, Trainable params: {trainable_count}")
            else:
                # è§£å†»é˜¶æ®µï¼šæ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
                for param in self.model.parameters():
                    param.requires_grad = True
                
                if epoch == curriculum_epochs + 1:
                    self.logger.info(f"ğŸ“š Curriculum: Unfreezing All Parameters (Epoch {epoch})")
                    self.logger.info(f"   All {sum(1 for _ in self.model.parameters())} parameters are now trainable")
        
        elif self.curriculum_mode == 'alpha_blending':
            # ====== ç­–ç•¥ B: æ¸è¿›å¼ Alpha æ··åˆ ======
            # è®¡ç®—å½“å‰ alpha å€¼ï¼šä» 1/curriculum_epochs æ¸è¿›åˆ° 1.0
            alpha = min(1.0, epoch / curriculum_epochs)
            
            # è°ƒç”¨æ¨¡å‹çš„ set_moe_alpha æ–¹æ³•
            if hasattr(self.model, 'set_moe_alpha'):
                self.model.set_moe_alpha(alpha)
            
            self.logger.info(f"ğŸ“š Curriculum: Setting MoE Alpha to {alpha:.4f} (Epoch {epoch}/{curriculum_epochs})")
            
            # ç¡®ä¿æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
            for param in self.model.parameters():
                param.requires_grad = True
    
    def log_metrics_to_file(self, epoch: int, train_metrics: Dict[str, float], 
                           valid_metrics: Dict[str, float], test_metrics: Dict[str, float] = None):
        """è®°å½•æ¯ä¸ªepochçš„æŒ‡æ ‡åˆ°txtæ–‡ä»¶"""
        if self.metrics_file is None:
            return
        
        try:
            with open(self.metrics_file, 'a') as f:
                f.write(f"{'='*60}\n")
                f.write(f"Epoch {epoch}/{self.config.num_epochs}\n")
                f.write(f"{'='*60}\n\n")
                
                # è®°å½•è®­ç»ƒé›†æŒ‡æ ‡
                f.write("è®­ç»ƒé›† (Train):\n")
                for key, value in train_metrics.items():
                    if isinstance(value, float):
                        f.write(f"  {key}: {value:.4f}\n")
                    else:
                        f.write(f"  {key}: {value}\n")
                f.write("\n")
                
                # è®°å½•éªŒè¯é›†æŒ‡æ ‡
                f.write("éªŒè¯é›† (Valid):\n")
                for key, value in valid_metrics.items():
                    if isinstance(value, float):
                        f.write(f"  {key}: {value:.4f}\n")
                    else:
                        f.write(f"  {key}: {value}\n")
                f.write("\n")
                
                # è®°å½•æµ‹è¯•é›†æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
                if test_metrics is not None:
                    f.write("æµ‹è¯•é›† (Test):\n")
                    for key, value in test_metrics.items():
                        if isinstance(value, float):
                            f.write(f"  {key}: {value:.4f}\n")
                        else:
                            f.write(f"  {key}: {value}\n")
                    f.write("\n")
                
                f.write("\n")
        except Exception as e:
            self.logger.warning(f"æ— æ³•å†™å…¥æŒ‡æ ‡æ–‡ä»¶: {e}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        # â­ åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼ˆåœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶ï¼‰
        self._apply_curriculum(epoch)
        
        self.model.train()
        
        loss_meter = AverageMeter()
        sphere_loss_meter = AverageMeter()
        moe_loss_meter = AverageMeter()  # â­ MoEè´Ÿè½½å‡è¡¡æŸå¤±è®¡é‡å™¨
        
        pbar = tqdm(self.train_loader,
                    desc=f'Epoch {epoch}/{self.config.num_epochs} [Train]',
                    position=0,
                    leave=True,
                    ncols=120,  # å›ºå®šå®½åº¦ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
                    bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # ä¼˜åŒ–æ ¼å¼
        
        all_preds = []
        all_labels = []
        
        # ====== æ§åˆ¶å…³é”®å¸§æ—¥å¿— ======
        if self.config.model_config.use_key_frame_selector:
            if hasattr(self.model, 'key_frame_selector') and self.model.key_frame_selector is not None:
                enable_kf_log = getattr(self.config, 'enable_keyframe_logging', False)
                kfs_log_freq = getattr(self.config, 'keyframe_log_every', 32)
                
                if epoch == 1:
                    # ç¬¬1ä¸ªepochæ˜¾ç¤ºé…ç½®ä¿¡æ¯
                    self.logger.info(f"\n{'='*70}")
                    self.logger.info(f"ğŸ“Š å…³é”®å¸§é€‰æ‹©å™¨é…ç½®:")
                    self.logger.info(f"  use_key_frame_selector: {self.config.model_config.use_key_frame_selector}")
                    self.logger.info(f"  enable_keyframe_logging: {enable_kf_log}")
                    self.logger.info(f"  n_segments: {self.model.key_frame_selector.n_segments}")
                    self.logger.info(f"  frame_ratio: {self.model.key_frame_selector.frame_ratio}%")
                    
                    if enable_kf_log:
                        self.model.key_frame_selector.enable_logging = True
                        self.model.key_frame_selector.log_every = kfs_log_freq
                        self.model.key_frame_selector.logger = self.logger  # ä¼ é€’logger
                        self.logger.info(f"  âœ“ å…³é”®å¸§ç»Ÿè®¡å·²å¯ç”¨ (æ¯{kfs_log_freq}ä¸ªutteranceæ‰“å°)")
                    else:
                        self.model.key_frame_selector.enable_logging = False
                        self.logger.info(f"  â„¹ï¸  å…³é”®å¸§ç»Ÿè®¡å·²ç¦ç”¨")
                        self.logger.info(f"     å¯ç”¨æ–¹æ³•: ä¿®æ”¹train_unified.shä¸­çš„ENABLE_KEYFRAME_LOGGING=true")
                    self.logger.info(f"{'='*70}\n")
                else:
                    # ç¬¬2ä¸ªepochå¼€å§‹ç¦ç”¨æ—¥å¿—
                    self.model.key_frame_selector.enable_logging = False
        else:
            if epoch == 1:
                self.logger.info("â„¹ï¸  å…³é”®å¸§é€‰æ‹©å™¨å·²ç¦ç”¨")
        # ==========================================
        
        for batch_idx, batch in enumerate(pbar):
            
            # ====== æ¨¡æ€è´¡çŒ®åº¦åˆ†æ ======
            if (self.modality_analysis_enabled and 
                self.modality_analyzer is not None and
                epoch <= self.modality_analysis_epochs and
                batch_idx % self.analyze_modality_every == 0 and
                batch_idx > 0):
                
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"Epoch {epoch} | Batch {batch_idx}/{len(self.train_loader)} - æ¨¡æ€è´¡çŒ®åº¦åˆ†æ")
                self.logger.info(f"{'='*70}")
                
                try:
                    analysis_batch = {
                        'text': batch['text'].to(self.device),
                        'audio': batch['audio'].to(self.device),
                        'vision': batch['vision'].to(self.device),
                        'text_global': batch['text_global'].to(self.device),
                        'social': batch['social'].to(self.device),
                        'context': batch['context'].to(self.device),
                        'label': batch['label'].to(self.device),
                    }
                    
                    scores = self.modality_analyzer.comprehensive_analysis(
                        self.model, analysis_batch, self.criterion, 
                        analyze_gradient=False, analyze_variance=False
                    )
                    report = self.modality_analyzer.format_analysis_results(scores)
                    self.logger.info(report)
                except Exception as e:
                    self.logger.warning(f"æ¨¡æ€åˆ†æå¤±è´¥: {e}")
            # ==========================================
            
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
            text_seq = batch['text'].to(self.device)
            audio_seq = batch['audio'].to(self.device)
            video_seq = batch['vision'].to(self.device)
            text_global = batch['text_global'].to(self.device)
            social = batch['social'].to(self.device)
            context = batch['context'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            # â­ ä¿®å¤ï¼šä¸ºè¶…å›¾æä¾›æ­£ç¡®çš„batch_dia_lenï¼ˆæ¯ä¸ªæ ·æœ¬æ˜¯ç‹¬ç«‹å¯¹è¯ï¼‰
            batch_dia_len_for_hypergraph = [1] * text_seq.size(0) if self.config.model_config.use_hypergraph else None
            
            logits, aux_outputs = self.model(
                text_sequence=text_seq,
                audio_sequence=audio_seq,
                video_sequence=video_seq,
                text_global=text_global,
                social_embedding=social,
                context_embedding=context,
                batch_dia_len=batch_dia_len_for_hypergraph,  # â­ ä¿®å¤ï¼šä¼ å…¥æ­£ç¡®çš„å¯¹è¯é•¿åº¦
            )
            
            # è®¡ç®—æŸå¤±
            if self.config.task_type == 'regression':
                loss = self.criterion(logits.squeeze(-1), labels.squeeze(-1))
            else:
                loss = self.criterion(logits, labels.long().squeeze())
            
            # æ·»åŠ è¶…çƒé¢æ­£åˆ™åŒ–æŸå¤±ï¼ˆå¯èƒ½å·²å¼ƒç”¨ï¼‰
            sphere_loss = aux_outputs['sphere_loss']
            
            # â­ æ·»åŠ MoEè´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆç‹¬ç«‹äºsphere_lossï¼‰
            moe_loss = aux_outputs.get('moe_loss', torch.tensor(0.0, device=loss.device))
            moe_loss_weight = getattr(self.config, 'moe_loss_weight', 0.0)
            
            # è®¡ç®—æ€»æŸå¤±
            total_loss = loss + self.config.sphere_loss_weight * sphere_loss + moe_loss_weight * moe_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)
            
            self.optimizer.step()
            
            # æ›´æ–°ç»Ÿè®¡
            loss_meter.update(loss.item(), text_seq.size(0))
            sphere_loss_meter.update(sphere_loss.item(), text_seq.size(0))
            moe_loss_meter.update(moe_loss.item(), text_seq.size(0))  # â­ æ›´æ–°MoEæŸå¤±
            
            # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾
            all_preds.append(logits.detach().cpu())
            all_labels.append(labels.detach().cpu())
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆåŒ…å«MoEæŸå¤±ï¼‰
            pbar.set_postfix({
                'loss': f'{loss_meter.avg:.4f}',
                'moe': f'{moe_loss_meter.avg:.4f}',  # â­ æ˜¾ç¤ºMoEæŸå¤±
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
            })
        
        # è®¡ç®—æŒ‡æ ‡
        all_preds = torch.cat(all_preds, dim=0).numpy()
        all_labels = torch.cat(all_labels, dim=0).numpy()
        
        if self.config.task_type == 'regression':
            if self.config.metrics_type == 'chsims':
                # åœ¨ç¬¬ä¸€ä¸ªepochå¯ç”¨è°ƒè¯•è¾“å‡º
                debug_mode = (epoch == 1)
                metrics = self.metrics_calc.calc_chsims_metrics(all_preds.squeeze(), all_labels.squeeze(), debug=debug_mode)
            else:
                metrics = self.metrics_calc.calc_regression_metrics(all_preds.squeeze(), all_labels.squeeze())
        else:
            pred_classes = all_preds.argmax(axis=1)
            if self.config.metrics_type == 'meld':
                # MELDä¸“ç”¨æŒ‡æ ‡ï¼šåŒ…å«æ¯ä¸ªæƒ…æ„Ÿç±»åˆ«çš„ACC/F1
                metrics = self.metrics_calc.calc_meld_metrics(pred_classes, all_labels.squeeze().astype(int))
            else:
                metrics = self.metrics_calc.calc_classification_metrics(pred_classes, all_labels.squeeze(), self.config.num_labels)
        
        metrics['loss'] = loss_meter.avg
        metrics['sphere_loss'] = sphere_loss_meter.avg
        metrics['moe_loss'] = moe_loss_meter.avg  # â­ è®°å½•MoEè´Ÿè½½å‡è¡¡æŸå¤±
        
        return metrics
    
    def evaluate(self, dataloader, split='valid') -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        loss_meter = AverageMeter()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            pbar = tqdm(dataloader, 
                       desc=f'{split.capitalize()} ', 
                       leave=False,
                       ncols=120,
                       bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')
            
            for batch in pbar:
                text_seq = batch['text'].to(self.device)
                audio_seq = batch['audio'].to(self.device)
                video_seq = batch['vision'].to(self.device)
                text_global = batch['text_global'].to(self.device)
                social = batch['social'].to(self.device)
                context = batch['context'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # â­ ä¿®å¤ï¼šä¸ºè¶…å›¾æä¾›æ­£ç¡®çš„batch_dia_len
                batch_dia_len_for_hypergraph = [1] * text_seq.size(0) if self.config.model_config.use_hypergraph else None
                
                logits, aux_outputs = self.model(
                    text_sequence=text_seq,
                    audio_sequence=audio_seq,
                    video_sequence=video_seq,
                    text_global=text_global,
                    social_embedding=social,
                    context_embedding=context,
                    batch_dia_len=batch_dia_len_for_hypergraph,  # â­ ä¿®å¤
                )
                
                if self.config.task_type == 'regression':
                    loss = self.criterion(logits.squeeze(-1), labels.squeeze(-1))
                else:
                    loss = self.criterion(logits, labels.long().squeeze())
                
                loss_meter.update(loss.item(), text_seq.size(0))
                
                all_preds.append(logits.detach().cpu())
                all_labels.append(labels.detach().cpu())
        
        # è®¡ç®—æŒ‡æ ‡
        all_preds = torch.cat(all_preds, dim=0).numpy()
        all_labels = torch.cat(all_labels, dim=0).numpy()
        
        if self.config.task_type == 'regression':
            if self.config.metrics_type == 'chsims':
                # åœ¨éªŒè¯é›†ç¬¬ä¸€æ¬¡è¯„ä¼°æ—¶å¯ç”¨è°ƒè¯•è¾“å‡º
                debug_mode = (split == 'valid' and not hasattr(self, '_debug_done'))
                if debug_mode:
                    self._debug_done = True
                metrics = self.metrics_calc.calc_chsims_metrics(all_preds.squeeze(), all_labels.squeeze(), debug=debug_mode)
            else:
                metrics = self.metrics_calc.calc_regression_metrics(all_preds.squeeze(), all_labels.squeeze())
        else:
            pred_classes = all_preds.argmax(axis=1)
            if self.config.metrics_type == 'meld':
                # MELDä¸“ç”¨æŒ‡æ ‡ï¼šåŒ…å«æ¯ä¸ªæƒ…æ„Ÿç±»åˆ«çš„ACC/F1
                metrics = self.metrics_calc.calc_meld_metrics(pred_classes, all_labels.squeeze().astype(int))
            else:
                metrics = self.metrics_calc.calc_classification_metrics(pred_classes, all_labels.squeeze(), self.config.num_labels)
        
        metrics['loss'] = loss_meter.avg
        
        return metrics
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("Starting training...")
        
        for epoch in range(1, self.config.num_epochs + 1):
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            
            # éªŒè¯
            valid_metrics = self.evaluate(self.valid_loader, 'valid')
            
            # æ—¥å¿—
            self.logger.info(f"\nEpoch {epoch}/{self.config.num_epochs}")
            self.logger.info(f"Train: {dict_to_str(train_metrics)}")
            self.logger.info(f"Valid: {dict_to_str(valid_metrics)}")
            
            # æ¯ä¸ªepochåè¯„ä¼°æµ‹è¯•é›†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            test_metrics_epoch = None
            if getattr(self.config, 'eval_test_every_epoch', False):
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"Epoch {epoch} - æµ‹è¯•é›†è¯„ä¼°ï¼ˆä»…ç›‘æ§ï¼Œä¸å½±å“æ—©åœï¼‰")
                self.logger.info(f"{'='*60}")
                test_metrics_epoch = self.evaluate(self.test_loader, split='test')
                self.logger.info(f"Test: {dict_to_str(test_metrics_epoch)}")
                self.logger.info(f"{'='*60}\n")
            
            # è®°å½•æŒ‡æ ‡åˆ°txtæ–‡ä»¶
            self.log_metrics_to_file(epoch, train_metrics, valid_metrics, test_metrics_epoch)
            
            # è®°å½•æŒ‡æ ‡åˆ°å†å²ï¼ˆç”¨äºç»˜å›¾ï¼‰
            self.metrics_history.update('train', epoch, train_metrics)
            self.metrics_history.update('valid', epoch, valid_metrics)
            if test_metrics_epoch is not None:
                self.metrics_history.update('test', epoch, test_metrics_epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                if self.config.scheduler_type == 'reduce_on_plateau':
                    monitor_metric = valid_metrics.get('mae', valid_metrics.get('loss'))
                    self.scheduler.step(monitor_metric)
                else:
                    self.scheduler.step()
            
            # æ—©åœæ£€æŸ¥
            # ä½¿ç”¨é…ç½®æŒ‡å®šçš„æŒ‡æ ‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°é»˜è®¤
            metric_name = getattr(self.config, 'early_stop_metric', 'mae')
            
            # â­ æ–°å¢ï¼šæ”¯æŒç»¼åˆæŒ‡æ ‡
            if metric_name == 'composite':
                # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆå½’ä¸€åŒ–ååŠ æƒï¼‰
                mae_score = -valid_metrics.get('MAE', 1.0)  # è´Ÿå€¼ï¼Œå› ä¸ºè¶Šå°è¶Šå¥½
                corr_score = valid_metrics.get('Corr', 0.0)
                acc5_score = valid_metrics.get('Acc_5', 0.0)
                
                monitor_metric = (
                    self.config.composite_mae_weight * mae_score +
                    self.config.composite_corr_weight * corr_score +
                    self.config.composite_acc5_weight * acc5_score
                )
                
                self.logger.info(f"  Composite Score: {monitor_metric:.4f} "
                               f"(MAE={mae_score:.3f}, Corr={corr_score:.3f}, Acc5={acc5_score:.3f})")
            else:
                # å¤„ç†å¤§å°å†™ä¸åŒ¹é…ï¼šutils.pyè¿”å›çš„é”®æ˜¯Acc_2, F1_2ç­‰ï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
                metric_name_variants = [
                    metric_name,  # åŸå§‹ï¼šacc_2
                    metric_name.upper(),  # å…¨å¤§å†™ï¼šACC_2
                    'Acc_2' if metric_name == 'acc_2' else metric_name,
                    'Acc_3' if metric_name == 'acc_3' else metric_name,
                    'F1_2' if metric_name == 'f1_2' else metric_name,
                    'F1_3' if metric_name == 'f1_3' else metric_name,
                    'F1_5' if metric_name == 'f1_5' else metric_name,
                    'Acc_5' if metric_name == 'acc_5' else metric_name,
                    'MAE' if metric_name == 'mae' else metric_name,
                    'Corr' if metric_name == 'corr' else metric_name,
                ]
                monitor_metric = None
                for variant in metric_name_variants:
                    if variant in valid_metrics:
                        monitor_metric = valid_metrics[variant]
                        break
                if monitor_metric is None:
                    monitor_metric = valid_metrics.get('MAE', valid_metrics.get('loss', 0.0))
            
            if self.config.metric_mode == 'min':
                is_best = monitor_metric < self.best_metric
            else:
                is_best = monitor_metric > self.best_metric
            
            if is_best:
                self.best_metric = monitor_metric
                self.best_epoch = epoch
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_path = os.path.join(self.config.save_dir, 'best_model.pth')
                save_checkpoint(
                    model=self.model,
                    optimizer=self.optimizer,
                    scheduler=self.scheduler,
                    epoch=epoch,
                    metrics=valid_metrics,
                    save_path=save_path,
                    is_best=True,
                )
                
                self.logger.info(f"âœ“ Best model saved! (metric: {self.best_metric:.4f})")
            
            # æ—©åœ
            self.early_stopping(monitor_metric, epoch)
            if self.early_stopping.early_stop:
                self.logger.info(f"Early stopping triggered at epoch {epoch}")
                break
        
        # æµ‹è¯•
        self.logger.info("\nEvaluating on test set...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint = torch.load(
            os.path.join(self.config.save_dir, 'best_model.pth'),
            map_location=self.device,
            weights_only=False  # PyTorch 2.6+ éœ€è¦æ­¤å‚æ•°
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        test_metrics = self.evaluate(self.test_loader, 'test')
        self.logger.info(f"Test: {dict_to_str(test_metrics)}")
        
        self.logger.info(f"\nTraining completed! Best epoch: {self.best_epoch}")
        self.logger.info(f"Best validation metric: {self.best_metric:.4f}")
        
        # ====== è®­ç»ƒç»“æŸåç»˜åˆ¶æ›²çº¿å›¾ ======
        if self.plotting_enabled and self.plotter is not None:
            self.logger.info("\n" + "="*60)
            self.logger.info("ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾...")
            self.logger.info("="*60)
            
            # åˆ¤æ–­æ˜¯å¦åŒ…å«æµ‹è¯•é›†æ•°æ®
            include_test = getattr(self.config, 'eval_test_every_epoch', False)
            
            # ç»˜åˆ¶å„ä¸ªæŒ‡æ ‡çš„ç‹¬ç«‹å›¾
            saved_paths = self.plotter.plot_all_metrics(
                self.metrics_history, 
                self.plot_config,
                include_test=include_test
            )
            
            # ç»˜åˆ¶ç»„åˆå›¾
            combined_path = self.plotter.plot_combined_figure(
                self.metrics_history,
                self.plot_config,
                include_test=include_test
            )
            
            # ä¿å­˜æŒ‡æ ‡å†å²åˆ°JSON
            history_path = os.path.join(self.config.save_dir, 'metrics_history.json')
            self.metrics_history.save_to_json(history_path)
            self.logger.info(f"âœ“ å·²ä¿å­˜æŒ‡æ ‡å†å²: {history_path}")
            
            self.logger.info("="*60 + "\n")
        # ==========================================


def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€è®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ')
    parser.add_argument('--dataset', type=str, required=True,
                        choices=['chsims', 'chsimsv2', 'meld'],
                        help='æ•°æ®é›†åç§°')
    parser.add_argument('--batch_size', type=int, default=None,
                        help='æ‰¹å¤§å°ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼ï¼‰')
    parser.add_argument('--learning_rate', type=float, default=None,
                        help='å­¦ä¹ ç‡ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼ï¼‰')
    parser.add_argument('--num_epochs', type=int, default=None,
                        help='è®­ç»ƒè½®æ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼ï¼‰')
    parser.add_argument('--data_dir', type=str, default=None,
                        help='æ•°æ®é›†ç›®å½•è·¯å¾„ï¼ˆè¦†ç›–é»˜è®¤è·¯å¾„ï¼‰')
    parser.add_argument('--early_stop_patience', type=int, default=None,
                        help='æ—©åœç­‰å¾…è½®æ•°ï¼ˆå¯é€‰ï¼Œ0=ç¦ç”¨æ—©åœï¼‰')
    parser.add_argument('--early_stop_metric', type=str, default=None,
                        choices=['mae', 'loss', 'acc_2', 'acc_3', 'acc_5', 'f1_2', 'f1_3', 'f1_5', 'corr', 'composite'],
                        help='æ—©åœç›‘æ§æŒ‡æ ‡ï¼ˆcompositeä¸ºç»¼åˆæŒ‡æ ‡ï¼š0.4*MAE + 0.3*Corr + 0.3*Acc5ï¼‰')
    parser.add_argument('--sphere_loss_weight', type=float, default=None,
                        help='è¶…çƒä½“æŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.01ï¼‰')
    parser.add_argument('--moe_loss_weight', type=float, default=None,
                        help='MoEè´Ÿè½½å‡è¡¡æŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.01ï¼Œé˜²æ­¢ä¸“å®¶åç¼©ï¼‰')
    
    # â­ è¯¾ç¨‹å­¦ä¹ å‚æ•°
    parser.add_argument('--curriculum_mode', type=str, default=None,
                        choices=['none', 'freeze_backbone', 'alpha_blending'],
                        help='è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ï¼šnone=å…³é—­ï¼Œfreeze_backbone=å†»ç»“éª¨å¹²ç½‘ç»œï¼Œalpha_blending=æ¸è¿›å¼MoEæ··åˆ')
    parser.add_argument('--curriculum_epochs', type=int, default=None,
                        help='è¯¾ç¨‹å­¦ä¹ æŒç»­çš„Epochæ•°ï¼ˆé»˜è®¤ï¼š5ï¼‰')
    
    parser.add_argument('--hidden_dim', type=int, default=None,
                        help='éšè—å±‚ç»´åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤256ï¼‰')
    parser.add_argument('--dropout_p', type=float, default=None,
                        help='Dropoutç‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.1ï¼‰')
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    parser.add_argument('--scheduler_type', type=str, default=None,
                        choices=['step', 'cosine', 'reduce_on_plateau', 'none'],
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤cosineï¼‰')
    parser.add_argument('--scheduler_gamma', type=float, default=None,
                        help='å­¦ä¹ ç‡è¡°å‡ç³»æ•°ï¼ˆé»˜è®¤0.5ï¼‰')
    parser.add_argument('--scheduler_patience', type=int, default=None,
                        help='ç­‰å¾…è½®æ•°ï¼ˆé»˜è®¤5ï¼Œä»…ç”¨äºreduce_on_plateauï¼‰')
    parser.add_argument('--scheduler_step_size', type=int, default=None,
                        help='æ­¥é•¿ï¼ˆé»˜è®¤10ï¼Œä»…ç”¨äºstepï¼‰')
    parser.add_argument('--warmup_ratio', type=float, default=None,
                        help='å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹ï¼ˆé»˜è®¤0.0å…³é—­ï¼Œ0.1=å‰10%æ­¥æ•°ç”¨äºé¢„çƒ­ï¼‰')
    
    # ç»„ä»¶å‚æ•°
    parser.add_argument('--n_key_frames', type=int, default=None,
                        help='å…³é”®å¸§æ•°é‡ï¼ˆå¯é€‰ï¼Œæ—§ç‰ˆå‚æ•°ï¼Œä¼˜å…ˆçº§ä½äºn_segments/frame_ratioï¼‰')
    parser.add_argument('--key_frame_segment_size', type=int, default=None,
                        help='å…³é”®å¸§åˆ†æ®µå¤§å°ï¼ˆå¯é€‰ï¼Œæ—§ç‰ˆå‚æ•°ï¼Œä¼˜å…ˆçº§ä½äºn_segments/frame_ratioï¼‰')
    # æ–°çš„ç™¾åˆ†æ¯”æ¨¡å¼å‚æ•°
    parser.add_argument('--n_segments', type=int, default=None,
                        help='MDP3åˆ†æ®µæ•°ï¼ˆé»˜è®¤ï¼šconfig.model_config.n_segmentsï¼‰')
    parser.add_argument('--frame_ratio', type=int, default=None,
                        help='æ¯æ®µé€‰æ‹©çš„å¸§ç™¾åˆ†æ¯”ï¼Œ1-100ï¼ˆé»˜è®¤ï¼šconfig.model_config.frame_ratioï¼‰')
    parser.add_argument('--num_film_experts', type=int, default=None,
                        help='MoE-FiLMä¸“å®¶æ•°é‡ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--film_top_k', type=int, default=None,
                        help='FiLM Top-Ké€‰æ‹©ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--num_hypergraph_layers', type=int, default=None,
                        help='è¶…å›¾å±‚æ•°ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--num_fourier_layers', type=int, default=None,
                        help='å‚…é‡Œå¶å±‚æ•°ï¼ˆå¯é€‰ï¼‰')
    
    # æ¨¡æ€å’Œå…³é”®å¸§åˆ†æå‚æ•° â­ æ–°å¢
    parser.add_argument('--enable_modality_analysis', action='store_true',
                        help='å¯ç”¨æ¨¡æ€è´¡çŒ®åº¦åˆ†æï¼ˆä¼šå½±å“è®­ç»ƒé€Ÿåº¦ï¼‰')
    parser.add_argument('--analyze_modality_every', type=int, default=None,
                        help='æ¯Nä¸ªbatchè¿›è¡Œæ¨¡æ€åˆ†æï¼ˆé»˜è®¤ï¼š10ï¼Œé€‚åº”å°æ•°æ®é›†ï¼‰')
    parser.add_argument('--keyframe_log_every', type=int, default=None,
                        help='æ¯Nä¸ªutteranceæ‰“å°å…³é”®å¸§ç»Ÿè®¡ï¼ˆé»˜è®¤ï¼š32ï¼‰')
    parser.add_argument('--modality_analysis_epochs', type=int, default=None,
                        help='åœ¨å‰Nä¸ªepochè¿›è¡Œæ¨¡æ€åˆ†æï¼ˆé»˜è®¤ï¼š3ï¼‰')
    parser.add_argument('--enable_keyframe_logging', action='store_true',
                        help='å¯ç”¨å…³é”®å¸§ç»Ÿè®¡æ‰“å°')
    parser.add_argument('--eval_test_every_epoch', action='store_true',
                        help='æ¯ä¸ªepochååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆä»…ç›‘æ§ï¼Œä¸å½±å“æ—©åœï¼‰')
    
    # ç»„ä»¶å¼€å…³ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰
    parser.add_argument('--no_key_frame_selector', action='store_true',
                        help='ç¦ç”¨å…³é”®å¸§é€‰æ‹©')
    parser.add_argument('--no_coupled_mamba', action='store_true',
                        help='ç¦ç”¨Coupled Mambaï¼ˆä½¿ç”¨ç‹¬ç«‹Mambaï¼‰')
    parser.add_argument('--no_moe_film', action='store_true',
                        help='ç¦ç”¨MoE-FiLMè°ƒåˆ¶')
    parser.add_argument('--no_hypergraph', action='store_true',
                        help='ç¦ç”¨è¶…å›¾å»ºæ¨¡')
    parser.add_argument('--no_frequency_decomp', action='store_true',
                        help='ç¦ç”¨é¢‘åŸŸåˆ†è§£')
    parser.add_argument('--no_sphere_reg', action='store_true',
                        help='ç¦ç”¨è¶…çƒä½“æ­£åˆ™åŒ–')
    parser.add_argument('--no_direct_fusion_priors', action='store_true',
                        help='ç¦æ­¢social/contextç›´æ¥å‚ä¸èåˆï¼ˆåªç”¨äºFiLMè°ƒåˆ¶ï¼‰')
    parser.add_argument('--use_improved_mlp', action='store_true',
                        help='ä½¿ç”¨æ”¹è¿›ç‰ˆMLPï¼ˆ4å±‚æ·±å±‚+GELU+æ®‹å·®+LayerNormï¼‰')
    parser.add_argument('--mlp_dropout', type=float, default=0.2,
                        help='æ”¹è¿›ç‰ˆMLPçš„Dropoutæ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼‰')
    parser.add_argument('--mlp_expansion_ratio', type=int, default=4,
                        help='æ”¹è¿›ç‰ˆMLPä¸­é—´å±‚æ‰©ç»´å€æ•°ï¼ˆé»˜è®¤4ï¼‰')
    
    # æŒ‡æ ‡è®°å½•æ–‡ä»¶
    parser.add_argument('--metrics_file', type=str, default=None,
                        help='æŒ‡æ ‡è®°å½•æ–‡ä»¶è·¯å¾„ï¼ˆtxtæ–‡ä»¶ï¼‰')
    
    # å¤šGPUæ”¯æŒï¼šè‡ªå®šä¹‰ä¿å­˜ç›®å½•
    parser.add_argument('--save_dir', type=str, default=None,
                        help='æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆç”¨äºå¤šGPUå¹¶è¡Œè®­ç»ƒé¿å…å†²çªï¼‰')
    parser.add_argument('--log_dir', type=str, default=None,
                        help='æ—¥å¿—ç›®å½•ï¼ˆç”¨äºå¤šGPUå¹¶è¡Œè®­ç»ƒé¿å…å†²çªï¼‰')
    
    # ====== è®­ç»ƒæ›²çº¿ç»˜å›¾å¼€å…³ ======
    parser.add_argument('--plot_mae', action='store_true',
                        help='ç»˜åˆ¶ MAE æ›²çº¿å›¾')
    parser.add_argument('--plot_acc2', action='store_true',
                        help='ç»˜åˆ¶ Acc-2 (äºŒåˆ†ç±»å‡†ç¡®ç‡) æ›²çº¿å›¾')
    parser.add_argument('--plot_acc3', action='store_true',
                        help='ç»˜åˆ¶ Acc-3 (ä¸‰åˆ†ç±»å‡†ç¡®ç‡) æ›²çº¿å›¾')
    parser.add_argument('--plot_acc5', action='store_true',
                        help='ç»˜åˆ¶ Acc-5 (äº”åˆ†ç±»å‡†ç¡®ç‡) æ›²çº¿å›¾')
    parser.add_argument('--plot_loss', action='store_true',
                        help='ç»˜åˆ¶ Loss æ›²çº¿å›¾')
    parser.add_argument('--plot_corr', action='store_true',
                        help='ç»˜åˆ¶ Correlation æ›²çº¿å›¾')
    parser.add_argument('--plot_all', action='store_true',
                        help='ç»˜åˆ¶æ‰€æœ‰æŒ‡æ ‡æ›²çº¿å›¾ï¼ˆç­‰åŒäºå¯ç”¨æ‰€æœ‰ --plot_* å¼€å…³ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    config = get_config(args.dataset)
    
    # è¦†ç›–åŸºç¡€å‚æ•°
    if args.data_dir is not None:
        config.data_dir = args.data_dir
    if args.batch_size is not None:
        config.batch_size = args.batch_size
    if args.learning_rate is not None:
        config.learning_rate = args.learning_rate
    if args.num_epochs is not None:
        config.num_epochs = args.num_epochs
    if args.early_stop_patience is not None:
        config.early_stop_patience = args.early_stop_patience
    if args.early_stop_metric is not None:
        config.early_stop_metric = args.early_stop_metric
    if args.sphere_loss_weight is not None:
        config.sphere_loss_weight = args.sphere_loss_weight
    if args.moe_loss_weight is not None:
        config.moe_loss_weight = args.moe_loss_weight
    
    # â­ è¯¾ç¨‹å­¦ä¹ å‚æ•°è¦†ç›–
    if args.curriculum_mode is not None:
        config.curriculum_mode = args.curriculum_mode
    if args.curriculum_epochs is not None:
        config.curriculum_epochs = args.curriculum_epochs
    
    if args.hidden_dim is not None:
        config.hidden_dim = args.hidden_dim
    if args.dropout_p is not None:
        config.dropout_p = args.dropout_p
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    if args.scheduler_type is not None:
        config.scheduler_type = args.scheduler_type
    if args.scheduler_gamma is not None:
        config.scheduler_gamma = args.scheduler_gamma
    if args.scheduler_patience is not None:
        config.scheduler_patience = args.scheduler_patience
    if args.scheduler_step_size is not None:
        config.scheduler_step_size = args.scheduler_step_size
    if args.warmup_ratio is not None:
        config.warmup_ratio = args.warmup_ratio
    
    # æ¨¡æ€å’Œå…³é”®å¸§åˆ†æå‚æ•°
    if args.enable_modality_analysis:
        config.enable_modality_analysis = True
    if args.analyze_modality_every is not None:
        config.analyze_modality_every = args.analyze_modality_every
    if args.keyframe_log_every is not None:
        config.keyframe_log_every = args.keyframe_log_every
    if args.modality_analysis_epochs is not None:
        config.modality_analysis_epochs = args.modality_analysis_epochs
    if args.enable_keyframe_logging:
        config.enable_keyframe_logging = True
    if args.eval_test_every_epoch:
        config.eval_test_every_epoch = True
    
    # åº”ç”¨ç»„ä»¶å‚æ•°
    if args.n_segments is not None:
        config.model_config.n_segments = max(1, args.n_segments)
    if args.frame_ratio is not None:
        if not (1 <= args.frame_ratio <= 100):
            raise ValueError("--frame_ratio å¿…é¡»åœ¨1åˆ°100ä¹‹é—´")
        config.model_config.frame_ratio = args.frame_ratio
    # å…¼å®¹æ—§å‚æ•°ï¼šè‹¥ç”¨æˆ·ä»ç„¶ä½¿ç”¨n_key_frames/key_frame_segment_sizeï¼Œåˆ™å›é€€åˆ°å›ºå®šå¸§æ•°æ¨¡å¼
    if args.n_key_frames is not None:
        config.model_config.n_key_frames = args.n_key_frames
    if args.key_frame_segment_size is not None:
        config.model_config.key_frame_segment_size = args.key_frame_segment_size
    if args.num_film_experts is not None:
        config.model_config.num_film_experts = args.num_film_experts
    if args.film_top_k is not None:
        config.model_config.film_top_k = args.film_top_k
    if args.num_hypergraph_layers is not None:
        config.model_config.num_hypergraph_layers = args.num_hypergraph_layers
    if args.num_fourier_layers is not None:
        config.model_config.num_fourier_layers = args.num_fourier_layers
    
    # åº”ç”¨ç»„ä»¶å¼€å…³
    if args.no_key_frame_selector:
        config.model_config.use_key_frame_selector = False
    if args.no_coupled_mamba:
        config.model_config.use_coupled_mamba = False
    if args.no_moe_film:
        config.model_config.use_moe_film = False
    if args.no_hypergraph:
        config.model_config.use_hypergraph = False
    if args.no_frequency_decomp:
        config.model_config.use_frequency_decomp = False
    if args.no_sphere_reg:
        config.model_config.use_sphere_regularization = False
    if args.no_direct_fusion_priors:
        config.model_config.direct_fusion_priors = False
    if args.use_improved_mlp:
        config.model_config.use_improved_mlp = True
    if args.mlp_dropout is not None:
        config.model_config.mlp_dropout = args.mlp_dropout
    if args.mlp_expansion_ratio is not None:
        config.model_config.mlp_expansion_ratio = args.mlp_expansion_ratio
    
    # æŒ‡æ ‡è®°å½•æ–‡ä»¶
    if args.metrics_file is not None:
        config.metrics_file = args.metrics_file
    
    # å¤šGPUæ”¯æŒï¼šè‡ªå®šä¹‰ä¿å­˜ç›®å½•å’Œæ—¥å¿—ç›®å½•
    if args.save_dir is not None:
        config.save_dir = args.save_dir
    if args.log_dir is not None:
        config.log_dir = args.log_dir
    
    # ====== ç»˜å›¾å¼€å…³ ======
    if args.plot_all:
        # --plot_all å¯ç”¨æ‰€æœ‰ç»˜å›¾
        config.plot_mae = True
        config.plot_acc2 = True
        config.plot_acc3 = True
        config.plot_acc5 = True
        config.plot_loss = True
        config.plot_corr = True
    else:
        # å•ç‹¬çš„ç»˜å›¾å¼€å…³
        config.plot_mae = args.plot_mae
        config.plot_acc2 = args.plot_acc2
        config.plot_acc3 = args.plot_acc3
        config.plot_acc5 = args.plot_acc5
        config.plot_loss = args.plot_loss
        config.plot_corr = args.plot_corr
    
    # è®­ç»ƒ
    trainer = Trainer(config)
    trainer.train()


if __name__ == '__main__':
    main()

