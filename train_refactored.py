# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€è®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ
æ”¯æŒ chsims/chsimsv2/meld æ•°æ®é›†
"""

import os
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm
from typing import Dict

from config_refactored import get_config
from data_loader_refactored import create_dataloaders_refactored, create_dialogue_dataloaders
from smsa_refactored import MultimodalEmotionModel_Refactored
from utils import (
    setup_seed, setup_logger, dict_to_str,
    MetricsCalculator, EarlyStopping,
    save_checkpoint, count_parameters,
    save_config, AverageMeter,
    MetricsHistory, TrainingPlotter
)

# å°è¯•å¯¼å…¥æ¨¡æ€åˆ†æå™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from modality_contribution_analyzer import ModalityContributionAnalyzer
    MODALITY_ANALYZER_AVAILABLE = True
except ImportError:
    MODALITY_ANALYZER_AVAILABLE = False


class Trainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = config.device
        
        # è®¾ç½®éšæœºç§å­
        setup_seed(config.seed)
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(config.log_dir, 'train')
        self.logger.info(str(config))
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(config.save_dir, exist_ok=True)
        
        # è®¾ç½®æŒ‡æ ‡è®°å½•æ–‡ä»¶
        self.metrics_file = getattr(config, 'metrics_file', None)
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.setup_data()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.setup_model()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.setup_optimizer()
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.setup_criterion()
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨å’Œæ—©åœ
        self.metrics_calc = MetricsCalculator()
        self.early_stopping = EarlyStopping(
            patience=config.early_stop_patience,
            mode=config.metric_mode,
            verbose=True,
        )
        
        self.best_metric = float('inf') if config.metric_mode == 'min' else float('-inf')
        self.best_epoch = 0
        
        # ====== æ¨¡æ€è´¡çŒ®åº¦åˆ†æå™¨ ======
        enable_analysis = getattr(config, 'enable_modality_analysis', False)
        if MODALITY_ANALYZER_AVAILABLE and enable_analysis:
            self.modality_analyzer = ModalityContributionAnalyzer(modalities=['social', 'context'])
            self.analyze_modality_every = getattr(config, 'analyze_modality_every', 10)
            self.modality_analysis_enabled = True
            self.modality_analysis_epochs = getattr(config, 'modality_analysis_epochs', 3)
            self.logger.info(f"âœ“ æ¨¡æ€åˆ†æå™¨å·²å¯ç”¨ (æ¯{self.analyze_modality_every}ä¸ªbatchåˆ†æ)")
            self.logger.info(f"  âš ï¸  æ³¨æ„ï¼šå³æ—¶æ¶ˆèåˆ†æä¼šå½±å“è®­ç»ƒé€Ÿåº¦")
        else:
            self.modality_analyzer = None
            self.modality_analysis_enabled = False
            if not enable_analysis:
                self.logger.info("â„¹ï¸  æ¨¡æ€åˆ†æå·²ç¦ç”¨ï¼ˆå¯é€šè¿‡ --enable_modality_analysis å¯ç”¨ï¼‰")
            elif not MODALITY_ANALYZER_AVAILABLE:
                self.logger.info("âš ï¸  æ¨¡æ€åˆ†æå™¨æœªå®‰è£…ï¼ˆmodality_contribution_analyzer.pyï¼‰")
        # ==========================================
        
        # ====== è®­ç»ƒæ›²çº¿ç»˜å›¾ ======
        self.metrics_history = MetricsHistory()
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç»˜å›¾
        self.plotting_enabled = getattr(config, 'plotting_enabled', False)
        
        if self.plotting_enabled:
            # æ ¹æ®ä»»åŠ¡ç±»å‹æ„å»ºç»˜å›¾é…ç½®
            if config.task_type == 'regression':
                # å›å½’ä»»åŠ¡æŒ‡æ ‡
                self.plot_config = {
                    'mae': getattr(config, 'plot_mae', True),
                    'loss': getattr(config, 'plot_loss', True),
                    'corr': getattr(config, 'plot_corr', True),
                    'acc_2': getattr(config, 'plot_acc2', True),
                    'acc_3': getattr(config, 'plot_acc3', True),
                    'acc_5': getattr(config, 'plot_acc5', True),
                }
            else:
                # åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡
                self.plot_config = {
                    'loss': getattr(config, 'plot_loss', True),
                    'Acc': getattr(config, 'plot_acc', True),
                    'F1_weighted': getattr(config, 'plot_f1_weighted', True),
                    'F1_macro': getattr(config, 'plot_f1_macro', True),
                }
            
            self.plotter = TrainingPlotter(config.save_dir, self.logger)
            enabled_plots = [k for k, v in self.plot_config.items() if v]
            self.logger.info(f"âœ“ è®­ç»ƒæ›²çº¿ç»˜å›¾å·²å¯ç”¨: {', '.join(enabled_plots)}")
        else:
            self.plot_config = {}
            self.plotter = None
            self.logger.info("â„¹ï¸  è®­ç»ƒæ›²çº¿ç»˜å›¾å·²ç¦ç”¨ï¼ˆå¯é€šè¿‡ --enable_plotting å¯ç”¨ï¼‰")
        # ==========================================
        
        # ====== è¯¾ç¨‹å­¦ä¹ é…ç½® ======
        self.curriculum_mode = getattr(config, 'curriculum_mode', 'none')
        self.curriculum_epochs = getattr(config, 'curriculum_epochs', 5)
        if self.curriculum_mode != 'none':
            self.logger.info(f"âœ“ è¯¾ç¨‹å­¦ä¹ å·²å¯ç”¨: mode={self.curriculum_mode}, epochs={self.curriculum_epochs}")
        else:
            self.logger.info("â„¹ï¸  è¯¾ç¨‹å­¦ä¹ å·²ç¦ç”¨ï¼ˆå¯é€šè¿‡ --curriculum_mode å¯ç”¨ï¼‰")
        # ==========================================
        
        # ====== æ··åˆå›æ”¾æ±  (Experience Replay) ======
        self.use_replay_buffer = getattr(config, 'use_replay_buffer', False)
        self.replay_buffer_threshold = getattr(config, 'replay_buffer_threshold', 1.5)  # lossé˜ˆå€¼
        self.replay_buffer_ratio = getattr(config, 'replay_buffer_ratio', 0.2)  # å›æ”¾æ¯”ä¾‹
        self.replay_buffer_max_size = getattr(config, 'replay_buffer_max_size', 500)  # æœ€å¤§å®¹é‡
        
        if self.use_replay_buffer:
            self.replay_buffer = []  # å­˜å‚¨é«˜losså¯¹è¯
            self.logger.info(f"âœ“ æ··åˆå›æ”¾æ± å·²å¯ç”¨:")
            self.logger.info(f"  - Lossé˜ˆå€¼: {self.replay_buffer_threshold}")
            self.logger.info(f"  - å›æ”¾æ¯”ä¾‹: {self.replay_buffer_ratio*100:.0f}%")
            self.logger.info(f"  - æœ€å¤§å®¹é‡: {self.replay_buffer_max_size}")
        else:
            self.replay_buffer = None
            self.logger.info("â„¹ï¸  æ··åˆå›æ”¾æ± å·²ç¦ç”¨ï¼ˆå¯é€šè¿‡ --use_replay_buffer å¯ç”¨ï¼‰")
        # ==========================================
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        self.logger.info("Loading data...")
        self.logger.info(f"Dataset: {self.config.dataset_name}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¯¹è¯çº§ batchingï¼ˆè¶…å›¾å»ºæ¨¡éœ€è¦ï¼‰
        use_dialogue_batching = getattr(self.config, 'use_dialogue_batching', False)
        
        if use_dialogue_batching:
            self.logger.info("âš ï¸  ä½¿ç”¨å¯¹è¯çº§ Batchingï¼ˆè¶…å›¾å»ºæ¨¡æ¨¡å¼ï¼‰")
            dialogue_batch_size = getattr(self.config, 'dialogue_batch_size', 8)
            max_dialogue_len = getattr(self.config.model_config, 'max_dialogue_len', 50)
            
            # è·å–æ¯æ‰¹æœ€å¤§ utterance æ•°ï¼ˆæ§åˆ¶æ˜¾å­˜ï¼‰
            max_utterances_per_batch = getattr(self.config, 'max_utterances_per_batch', 128)
            
            self.train_loader, self.valid_loader, self.test_loader = create_dialogue_dataloaders(
                data_dir=self.config.data_dir,
                num_workers=self.config.num_workers,
                seq_length=self.config.seq_length,
                augment_train=self.config.augment_train,
                noise_scale=self.config.noise_scale,
                max_dialogue_len=max_dialogue_len,
                max_utterances_per_batch=max_utterances_per_batch,
            )
            
            self.logger.info(f"Train dialogues: {len(self.train_loader.dataset)}")
            self.logger.info(f"Max utterances per batch: {max_utterances_per_batch}")
            self.logger.info(f"Valid dialogues: {len(self.valid_loader.dataset)}")
            self.logger.info(f"Test dialogues: {len(self.test_loader.dataset)}")
            self.logger.info(f"Dialogue batch size: {dialogue_batch_size}")
            self.logger.info(f"Max dialogue len: {max_dialogue_len}")
        else:
            # åŸå§‹ utterance çº§ batching
            self.train_loader, self.valid_loader, self.test_loader = create_dataloaders_refactored(
                data_dir=self.config.data_dir,
                batch_size=self.config.batch_size,
                num_workers=self.config.num_workers,
                seq_length=self.config.seq_length,
                augment_train=self.config.augment_train,
                noise_scale=self.config.noise_scale,
                cache_size=self.config.cache_size,
                use_weighted_sampler=getattr(self.config, 'use_weighted_sampler', False),
            )
            
            self.logger.info(f"Train samples: {len(self.train_loader.dataset)}")
            self.logger.info(f"Valid samples: {len(self.valid_loader.dataset)}")
            self.logger.info(f"Test samples: {len(self.test_loader.dataset)}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        self.logger.info("Initializing model...")
        
        self.model = MultimodalEmotionModel_Refactored(
            text_input_dim=self.config.text_input_dim,
            audio_input_dim=self.config.audio_input_dim,
            video_input_dim=self.config.video_input_dim,
            text_global_dim=self.config.text_global_dim,
            social_dim=self.config.social_dim,
            context_dim=self.config.context_dim,
            hidden_dim=self.config.hidden_dim,
            model_config=self.config.model_config,
            num_ism_layers=self.config.num_ism_layers,
            num_coupled_layers=self.config.num_coupled_layers,
            num_labels=self.config.num_labels,
            fusion_hidden_dim=self.config.fusion_hidden_dim,
            dropout_p=self.config.dropout_p,
        ).to(self.device)
        
        num_params = count_parameters(self.model)
        self.logger.info(f"Model parameters: {num_params:,}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
        )
        
        # è®¡ç®— warmup æ­¥æ•°
        warmup_ratio = getattr(self.config, 'warmup_ratio', 0.0)
        warmup_epochs = int(self.config.num_epochs * warmup_ratio)
        
        if self.config.scheduler_type == 'step':
            base_scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.scheduler_step_size,
                gamma=self.config.scheduler_gamma,
            )
        elif self.config.scheduler_type == 'cosine':
            base_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.num_epochs - warmup_epochs,  # ä½™å¼¦é€€ç«é˜¶æ®µçš„æ€»é•¿åº¦
            )
        elif self.config.scheduler_type == 'reduce_on_plateau':
            # ReduceLROnPlateau ä¸æ”¯æŒ SequentialLRï¼Œå•ç‹¬å¤„ç†
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode=self.config.metric_mode,
                factor=self.config.scheduler_gamma,
                patience=self.config.scheduler_patience,
            )
            if warmup_epochs > 0:
                self.logger.warning(f"âš  ReduceLROnPlateau ä¸æ”¯æŒ Warmupï¼Œå·²å¿½ç•¥ warmup_ratio={warmup_ratio}")
            return
        else:
            self.scheduler = None
            return
        
        # å¦‚æœå¯ç”¨äº† warmupï¼Œä½¿ç”¨ SequentialLR ç»„åˆ warmup + ä¸»è°ƒåº¦å™¨
        if warmup_epochs > 0:
            warmup_scheduler = optim.lr_scheduler.LinearLR(
                self.optimizer,
                start_factor=0.1,  # ä» 10% å­¦ä¹ ç‡å¼€å§‹
                end_factor=1.0,    # é¢„çƒ­åˆ° 100% å­¦ä¹ ç‡
                total_iters=warmup_epochs,
            )
            self.scheduler = optim.lr_scheduler.SequentialLR(
                self.optimizer,
                schedulers=[warmup_scheduler, base_scheduler],
                milestones=[warmup_epochs],
            )
            self.logger.info(f"âœ“ å¯ç”¨å­¦ä¹ ç‡é¢„çƒ­: {warmup_epochs} epochs (warmup_ratio={warmup_ratio})")
        else:
            self.scheduler = base_scheduler
    
    def setup_criterion(self):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        if self.config.task_type == 'regression':
            # å°è¯•ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
            loss_type = getattr(self.config, 'loss_function', 'mse')
            
            if loss_type == 'l1' or loss_type == 'mae':
                self.criterion = nn.L1Loss()
                self.logger.info("âœ“ ä½¿ç”¨ L1 Loss (MAE)")
            elif loss_type == 'focal_mse':
                try:
                    from losses import FocalMSELoss
                    self.criterion = FocalMSELoss(gamma=2.0)
                    self.logger.info("âœ“ ä½¿ç”¨ Focal MSE Loss (gamma=2.0)")
                except ImportError:
                    self.logger.warning("âš ï¸  losses.pyä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†MSE Loss")
                    self.criterion = nn.MSELoss()
            elif loss_type == 'huber':
                try:
                    from losses import HuberLoss
                    self.criterion = HuberLoss(delta=1.0)
                    self.logger.info("âœ“ ä½¿ç”¨ Huber Loss (delta=1.0)")
                except ImportError:
                    self.logger.warning("âš ï¸  losses.pyä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†MSE Loss")
                    self.criterion = nn.MSELoss()
            else:
                self.criterion = nn.MSELoss()
                self.logger.info("ä½¿ç”¨æ ‡å‡† MSE Loss")
        else:
            # â­ åˆ†ç±»ä»»åŠ¡ï¼šæ”¯æŒå¤šç§æŸå¤±å‡½æ•°å’Œç±»åˆ«æƒé‡
            loss_type = getattr(self.config, 'loss_function', 'ce')
            use_class_weights = getattr(self.config, 'use_class_weights', False)
            
            # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
            class_weights = None
            if use_class_weights and hasattr(self, 'train_loader'):
                class_counts = self._compute_class_distribution()
                if class_counts is not None:
                    total_samples = sum(class_counts)
                    num_classes = len(class_counts)
                    # åŸºç¡€é€†é¢‘ç‡æƒé‡ (ä½¿ç”¨sqrtå¹³æ»‘ï¼Œé˜²æ­¢æƒé‡è¿‡å¤§å¯¼è‡´è¿‡æ‹Ÿåˆ)
                    import math
                    weights = [math.sqrt(total_samples / (num_classes * max(c, 1))) for c in class_counts]
                    # å¦‚æœä¸å¸Œæœ›å¹³æ»‘ï¼Œå¯ä»¥ä½¿ç”¨åŸå§‹å…¬å¼ï¼š
                    # weights = [total_samples / (num_classes * max(c, 1)) for c in class_counts]
                    
                    class_weights = torch.tensor(weights, dtype=torch.float32).to(self.device)
                    self.logger.info(f"âœ“ å·²è®¡ç®—ç±»åˆ«æƒé‡(sqrtå¹³æ»‘): {[f'{w:.2f}' for w in weights]}")
                else:
                    self.logger.warning("âš ï¸  æ— æ³•è®¡ç®—ç±»åˆ«åˆ†å¸ƒï¼Œå°†ä¸ä½¿ç”¨è‡ªåŠ¨ç±»åˆ«æƒé‡")

            # æ ¹æ®é…ç½®é€‰æ‹©æŸå¤±å‡½æ•°
            if loss_type == 'focal':
                try:
                    from losses import FocalLoss
                    # FocalLoss å¯ä»¥æ¥æ”¶ alpha (ç±»åˆ«æƒé‡) å’Œ gamma
                    gamma = getattr(self.config, 'focal_gamma', 2.0)
                    label_smoothing = getattr(self.config, 'label_smoothing', 0.0)
                    
                    # â­ åŠ¨æ€gammaå‚æ•°
                    dynamic_gamma = getattr(self.config, 'focal_dynamic_gamma', False)
                    gamma_min = getattr(self.config, 'focal_gamma_min', 0.5)
                    gamma_decay_mode = getattr(self.config, 'focal_gamma_decay_mode', 'cosine')
                    
                    self.criterion = FocalLoss(
                        alpha=class_weights, 
                        gamma=gamma, 
                        label_smoothing=label_smoothing,
                        dynamic_gamma=dynamic_gamma,
                        gamma_min=gamma_min,
                        gamma_decay_mode=gamma_decay_mode
                    )
                    
                    if dynamic_gamma:
                        self.logger.info(f"âœ“ ä½¿ç”¨ Dynamic Focal Loss (gamma: {gamma:.1f}â†’{gamma_min:.1f}, "
                                       f"mode={gamma_decay_mode}, weighted={class_weights is not None}, smooth={label_smoothing})")
                    else:
                        self.logger.info(f"âœ“ ä½¿ç”¨ Focal Loss (gamma={gamma}, weighted={class_weights is not None}, smooth={label_smoothing})")
                except ImportError:
                    self.logger.warning("âš ï¸  losses.pyä¸å¯ç”¨ï¼Œå›é€€åˆ° CrossEntropyLoss")
                    self.criterion = nn.CrossEntropyLoss(weight=class_weights)
            else:
                # é»˜è®¤ä¸º CrossEntropyLoss
                label_smoothing = getattr(self.config, 'label_smoothing', 0.0)
                self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)
                if class_weights is not None:
                    self.logger.info(f"âœ“ ä½¿ç”¨å¸¦æƒé‡çš„ CrossEntropyLoss (smooth={label_smoothing})")
                else:
                    self.logger.info(f"ä½¿ç”¨æ ‡å‡† CrossEntropyLoss (smooth={label_smoothing})")
        
        # â­ KLæ•£åº¦å¤šä»»åŠ¡å­¦ä¹ ï¼šåˆå§‹åŒ–KLæŸå¤±å‡½æ•°
        use_kl_mtl = getattr(self.config, 'use_kl_mtl', False)
        if use_kl_mtl:
            try:
                from losses import MaskedKLDivLoss
                self.kl_criterion = MaskedKLDivLoss(reduction='batchmean')
                self.logger.info("âœ“ å¯ç”¨ KLæ•£åº¦å¤šä»»åŠ¡å­¦ä¹  (GS-MCCé£æ ¼)")
            except ImportError:
                self.logger.warning("âš ï¸  losses.pyä¸å¯ç”¨ï¼ŒKL MTLå°†è¢«ç¦ç”¨")
                self.config.use_kl_mtl = False
        else:
            self.kl_criterion = None
    
    def _compute_class_distribution(self):
        """è®¡ç®—è®­ç»ƒé›†çš„ç±»åˆ«åˆ†å¸ƒ"""
        try:
            num_classes = self.config.num_labels
            class_counts = [0] * num_classes
            
            # éå†è®­ç»ƒé›†ç»Ÿè®¡å„ç±»åˆ«æ ·æœ¬æ•°
            for batch in self.train_loader:
                labels = batch['label']
                for label in labels:
                    label_idx = int(label.item())
                    if 0 <= label_idx < num_classes:
                        class_counts[label_idx] += 1
            
            return class_counts
        except Exception as e:
            self.logger.warning(f"è®¡ç®—ç±»åˆ«åˆ†å¸ƒæ—¶å‡ºé”™: {e}")
            return None
    
    def _apply_curriculum(self, epoch: int) -> None:
        """
        åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥
        
        ç­–ç•¥ A: freeze_backbone
            - åœ¨è¯¾ç¨‹å­¦ä¹ æœŸé—´å†»ç»“ Mamba Backboneï¼Œåªè®­ç»ƒ MoE/FiLM/Head
            - è¯¾ç¨‹ç»“æŸåè§£å†»æ‰€æœ‰å‚æ•°
        
        ç­–ç•¥ B: alpha_blending
            - æ¸è¿›å¼å¢åŠ  MoE çš„å½±å“åŠ›
            - alpha = min(1.0, (epoch + 1) / curriculum_epochs)
            - æ‰€æœ‰å‚æ•°å§‹ç»ˆå¯è®­ç»ƒ
        
        â­ æ–°å¢ï¼šåŠ¨æ€gammaè¡°å‡
            - å¦‚æœcriterionæ˜¯FocalLossä¸”å¯ç”¨dynamic_gammaï¼Œä¼šè‡ªåŠ¨æ›´æ–°gammaå€¼
            - gammaä»åˆå§‹å€¼é€æ¸è¡°å‡åˆ°gamma_minï¼Œå¹³è¡¡å°‘æ•°ç±»å’Œå¤šæ•°ç±»
        
        Args:
            epoch: å½“å‰ epoch ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
        """
        # â­ æ›´æ–° Focal Loss çš„ gammaï¼ˆå¦‚æœå¯ç”¨äº†åŠ¨æ€gammaï¼‰
        if hasattr(self.criterion, 'dynamic_gamma') and self.criterion.dynamic_gamma:
            # epochä»1å¼€å§‹ï¼Œä½†update_gammaæœŸæœ›ä»0å¼€å§‹çš„ç´¢å¼•
            self.criterion.update_gamma(epoch - 1, self.config.num_epochs)
            self.logger.info(f"ğŸ“‰ Dynamic Focal Loss: gamma={self.criterion.gamma:.3f} "
                           f"(init={self.criterion.gamma_init:.1f}, min={self.criterion.gamma_min:.1f})")
        
        if self.curriculum_mode == 'none':
            return
        
        curriculum_epochs = self.curriculum_epochs
        
        if self.curriculum_mode == 'freeze_backbone':
            # ====== ç­–ç•¥ A: å†»ç»“éª¨å¹²ç½‘ç»œ ======
            if epoch <= curriculum_epochs:
                # å†»ç»“é˜¶æ®µï¼šåªè®­ç»ƒè°ƒåˆ¶æ¨¡å—å’Œåˆ†ç±»å¤´
                frozen_count = 0
                trainable_count = 0
                
                # â­ æ ¹æ®é…ç½®å†³å®šå¯è®­ç»ƒçš„è°ƒåˆ¶æ¨¡å—
                trainable_keys = [
                    'classifier', 'pre_classifier',  # åˆ†ç±»å¤´
                    'modality_fusion',  # èåˆå±‚
                    'freq_fusion', 'freq_decomp',  # é¢‘åŸŸåˆ†è§£
                ]
                
                # å¦‚æœå¯ç”¨äº† MoE-FiLMï¼Œæ·»åŠ  FiLM æ¨¡å—
                if self.config.model_config.use_moe_film:
                    trainable_keys.extend(['social_film', 'context_film'])
                
                # â­ å¦‚æœå¯ç”¨äº† DSPSï¼Œæ·»åŠ  DSPS æŠ•å½±å±‚
                if self.config.model_config.use_dsps:
                    trainable_keys.append('dsps_proj')
                
                for name, param in self.model.named_parameters():
                    # åˆ¤æ–­æ˜¯å¦æ˜¯å¯è®­ç»ƒæ¨¡å—çš„å‚æ•°
                    is_trainable_module = any(key in name for key in trainable_keys)
                    
                    if is_trainable_module:
                        param.requires_grad = True
                        trainable_count += 1
                    else:
                        param.requires_grad = False
                        frozen_count += 1
                
                self.logger.info(f"ğŸ“š Curriculum: Freezing Backbone (Epoch {epoch}/{curriculum_epochs})")
                self.logger.info(f"   Frozen params: {frozen_count}, Trainable params: {trainable_count}")
                self.logger.info(f"   Trainable modules: {trainable_keys}")
            else:
                # è§£å†»é˜¶æ®µï¼šæ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
                for param in self.model.parameters():
                    param.requires_grad = True
                
                if epoch == curriculum_epochs + 1:
                    self.logger.info(f"ğŸ“š Curriculum: Unfreezing All Parameters (Epoch {epoch})")
                    self.logger.info(f"   All {sum(1 for _ in self.model.parameters())} parameters are now trainable")
        
        elif self.curriculum_mode == 'alpha_blending':
            # ====== ç­–ç•¥ B: æ¸è¿›å¼ Alpha æ··åˆ ======
            # â­ æ³¨æ„ï¼šalpha_blending ä»…å¯¹ MoE-FiLM æœ‰æ•ˆ
            # å½“ MoE-FiLM å…³é—­æ—¶ï¼Œæ­¤ç­–ç•¥æ— æ•ˆæœï¼ˆä½†ä¸æŠ¥é”™ï¼Œåªæ˜¯è­¦å‘Šï¼‰
            
            if not self.config.model_config.use_moe_film:
                if epoch == 1:
                    self.logger.warning(
                        f"âš ï¸  Curriculum alpha_blending ä»…å¯¹ MoE-FiLM æœ‰æ•ˆï¼Œ"
                        f"å½“å‰ MoE-FiLM å·²å…³é—­ï¼Œæ­¤ç­–ç•¥å°†ä¸èµ·ä½œç”¨"
                    )
            else:
                # è®¡ç®—å½“å‰ alpha å€¼ï¼šä» 1/curriculum_epochs æ¸è¿›åˆ° 1.0
                alpha = min(1.0, epoch / curriculum_epochs)
                
                # è°ƒç”¨æ¨¡å‹çš„ set_moe_alpha æ–¹æ³•
                if hasattr(self.model, 'set_moe_alpha'):
                    self.model.set_moe_alpha(alpha)
                
                self.logger.info(f"ğŸ“š Curriculum: Setting MoE Alpha to {alpha:.4f} (Epoch {epoch}/{curriculum_epochs})")
            
            # ç¡®ä¿æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
            for param in self.model.parameters():
                param.requires_grad = True
    
    def log_metrics_to_file(self, epoch: int, train_metrics: Dict[str, float], 
                           valid_metrics: Dict[str, float], test_metrics: Dict[str, float] = None):
        """è®°å½•æ¯ä¸ªepochçš„æŒ‡æ ‡åˆ°txtæ–‡ä»¶"""
        if self.metrics_file is None:
            return
        
        try:
            with open(self.metrics_file, 'a') as f:
                f.write(f"{'='*60}\n")
                f.write(f"Epoch {epoch}/{self.config.num_epochs}\n")
                f.write(f"{'='*60}\n\n")
                
                # è®°å½•è®­ç»ƒé›†æŒ‡æ ‡
                f.write("è®­ç»ƒé›† (Train):\n")
                for key, value in train_metrics.items():
                    if isinstance(value, float):
                        f.write(f"  {key}: {value:.4f}\n")
                    else:
                        f.write(f"  {key}: {value}\n")
                f.write("\n")
                
                # è®°å½•éªŒè¯é›†æŒ‡æ ‡
                f.write("éªŒè¯é›† (Valid):\n")
                for key, value in valid_metrics.items():
                    if isinstance(value, float):
                        f.write(f"  {key}: {value:.4f}\n")
                    else:
                        f.write(f"  {key}: {value}\n")
                f.write("\n")
                
                # è®°å½•æµ‹è¯•é›†æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
                if test_metrics is not None:
                    f.write("æµ‹è¯•é›† (Test):\n")
                    for key, value in test_metrics.items():
                        if isinstance(value, float):
                            f.write(f"  {key}: {value:.4f}\n")
                        else:
                            f.write(f"  {key}: {value}\n")
                    f.write("\n")
                
                f.write("\n")
        except Exception as e:
            self.logger.warning(f"æ— æ³•å†™å…¥æŒ‡æ ‡æ–‡ä»¶: {e}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        # â­ åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼ˆåœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶ï¼‰
        self._apply_curriculum(epoch)
        
        self.model.train()
        
        loss_meter = AverageMeter()
        sphere_loss_meter = AverageMeter()
        moe_loss_meter = AverageMeter()  # â­ MoEè´Ÿè½½å‡è¡¡æŸå¤±è®¡é‡å™¨
        mtl_loss_meter = AverageMeter()  # â­ CH-SIMSv2 MTLæŸå¤±è®¡é‡å™¨
        
        pbar = tqdm(self.train_loader,
                    desc=f'Epoch {epoch}/{self.config.num_epochs} [Train]',
                    position=0,
                    leave=True,
                    ncols=120,  # å›ºå®šå®½åº¦ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
                    bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # ä¼˜åŒ–æ ¼å¼
        
        all_preds = []
        all_labels = []
        
        # ====== æ§åˆ¶å…³é”®å¸§æ—¥å¿— ======
        if self.config.model_config.use_key_frame_selector:
            if hasattr(self.model, 'key_frame_selector') and self.model.key_frame_selector is not None:
                enable_kf_log = getattr(self.config, 'enable_keyframe_logging', False)
                kfs_log_freq = getattr(self.config, 'keyframe_log_every', 32)
                
                if epoch == 1:
                    # ç¬¬1ä¸ªepochæ˜¾ç¤ºé…ç½®ä¿¡æ¯
                    self.logger.info(f"\n{'='*70}")
                    self.logger.info(f"ğŸ“Š å…³é”®å¸§é€‰æ‹©å™¨é…ç½®:")
                    self.logger.info(f"  use_key_frame_selector: {self.config.model_config.use_key_frame_selector}")
                    self.logger.info(f"  enable_keyframe_logging: {enable_kf_log}")
                    self.logger.info(f"  n_segments: {self.model.key_frame_selector.n_segments}")
                    self.logger.info(f"  frame_ratio: {self.model.key_frame_selector.frame_ratio}%")
                    
                    if enable_kf_log:
                        self.model.key_frame_selector.enable_logging = True
                        self.model.key_frame_selector.log_every = kfs_log_freq
                        self.model.key_frame_selector.logger = self.logger  # ä¼ é€’logger
                        self.logger.info(f"  âœ“ å…³é”®å¸§ç»Ÿè®¡å·²å¯ç”¨ (æ¯{kfs_log_freq}ä¸ªutteranceæ‰“å°)")
                    else:
                        self.model.key_frame_selector.enable_logging = False
                        self.logger.info(f"  â„¹ï¸  å…³é”®å¸§ç»Ÿè®¡å·²ç¦ç”¨")
                        self.logger.info(f"     å¯ç”¨æ–¹æ³•: ä¿®æ”¹train_unified.shä¸­çš„ENABLE_KEYFRAME_LOGGING=true")
                    self.logger.info(f"{'='*70}\n")
                else:
                    # ç¬¬2ä¸ªepochå¼€å§‹ç¦ç”¨æ—¥å¿—
                    self.model.key_frame_selector.enable_logging = False
        else:
            if epoch == 1:
                self.logger.info("â„¹ï¸  å…³é”®å¸§é€‰æ‹©å™¨å·²ç¦ç”¨")
        # ==========================================
        
        for batch_idx, batch in enumerate(pbar):
            
            # ====== æ¨¡æ€è´¡çŒ®åº¦åˆ†æ ======
            if (self.modality_analysis_enabled and 
                self.modality_analyzer is not None and
                epoch <= self.modality_analysis_epochs and
                batch_idx % self.analyze_modality_every == 0 and
                batch_idx > 0):
                
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"Epoch {epoch} | Batch {batch_idx}/{len(self.train_loader)} - æ¨¡æ€è´¡çŒ®åº¦åˆ†æ")
                self.logger.info(f"{'='*70}")
                
                try:
                    analysis_batch = {
                        'text': batch['text'].to(self.device),
                        'audio': batch['audio'].to(self.device),
                        'vision': batch['vision'].to(self.device),
                        'text_global': batch['text_global'].to(self.device),
                        'social': batch['social'].to(self.device),
                        'context': batch['context'].to(self.device),
                        'label': batch['label'].to(self.device),
                    }
                    
                    scores = self.modality_analyzer.comprehensive_analysis(
                        self.model, analysis_batch, self.criterion, 
                        analyze_gradient=False, analyze_variance=False
                    )
                    report = self.modality_analyzer.format_analysis_results(scores)
                    self.logger.info(report)
                except Exception as e:
                    self.logger.warning(f"æ¨¡æ€åˆ†æå¤±è´¥: {e}")
            # ==========================================
            
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
            text_seq = batch['text'].to(self.device)
            audio_seq = batch['audio'].to(self.device)
            video_seq = batch['vision'].to(self.device)
            text_global = batch['text_global'].to(self.device)
            social = batch['social'].to(self.device)
            context = batch['context'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # CH-SIMSv2 MTL: è¯»å–å•æ¨¡æ€æ ‡ç­¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            mtl_lambda = getattr(self.config, 'chsimsv2_mtl_lambda', 0.0)
            if mtl_lambda > 0 and 'label_T' in batch:
                labels_T = batch['label_T'].to(self.device)
                labels_A = batch['label_A'].to(self.device)
                labels_V = batch['label_V'].to(self.device)
            else:
                labels_T = labels_A = labels_V = None
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            # â­ è·å– batch_dia_lenï¼š
            # - å¯¹è¯çº§ batching æ—¶ï¼šä» batch ä¸­è·å–çœŸå®çš„å¯¹è¯é•¿åº¦åˆ—è¡¨
            # - utterance çº§ batching æ—¶ï¼šæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹ï¼Œä½¿ç”¨ [1, 1, ...] ä½œä¸º fallback
            if 'batch_dia_len' in batch:
                # å¯¹è¯çº§ batchingï¼šä½¿ç”¨çœŸå®çš„å¯¹è¯é•¿åº¦
                batch_dia_len_for_hypergraph = batch['batch_dia_len']
            elif self.config.model_config.use_hypergraph:
                # utterance çº§ batching + è¶…å›¾ï¼šfallback åˆ° [1] * batch_size
                batch_dia_len_for_hypergraph = [1] * text_seq.size(0)
            else:
                batch_dia_len_for_hypergraph = None
            
            logits, aux_outputs = self.model(
                text_sequence=text_seq,
                audio_sequence=audio_seq,
                video_sequence=video_seq,
                text_global=text_global,
                social_embedding=social,
                context_embedding=context,
                batch_dia_len=batch_dia_len_for_hypergraph,
            )
            
            # è®¡ç®—æŸå¤±
            if self.config.task_type == 'regression':
                loss = self.criterion(logits.squeeze(-1), labels.squeeze(-1))
            else:
                loss = self.criterion(logits, labels.long().squeeze())
            
            # æ·»åŠ è¶…çƒé¢æ­£åˆ™åŒ–æŸå¤±ï¼ˆå¯èƒ½å·²å¼ƒç”¨ï¼‰
            sphere_loss = aux_outputs['sphere_loss']
            
            # â­ æ·»åŠ MoEè´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆç‹¬ç«‹äºsphere_lossï¼‰
            moe_loss = aux_outputs.get('moe_loss', torch.tensor(0.0, device=loss.device))
            moe_loss_weight = getattr(self.config, 'moe_loss_weight', 0.0)
            
            # â­ CH-SIMSv2 MTL: è®¡ç®—å•æ¨¡æ€è¾…åŠ©æŸå¤±
            mtl_loss = torch.tensor(0.0, device=loss.device)
            if mtl_lambda > 0 and labels_T is not None:
                # ä» aux_outputs è·å–å•æ¨¡æ€è¡¨ç¤ºï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                # ç›®å‰å…ˆä½¿ç”¨ä¸»é¢„æµ‹ä½œä¸ºæ‰€æœ‰æ¨¡æ€çš„é¢„æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
                # TODO: æœªæ¥å¯åœ¨æ¨¡å‹ä¸­æ·»åŠ ä¸“é—¨çš„å•æ¨¡æ€é¢„æµ‹å¤´
                loss_T = self.criterion(logits.squeeze(-1), labels_T.squeeze(-1))
                loss_A = self.criterion(logits.squeeze(-1), labels_A.squeeze(-1))
                loss_V = self.criterion(logits.squeeze(-1), labels_V.squeeze(-1))
                mtl_loss = (loss_T + loss_A + loss_V) / 3.0
            
            # â­ KLæ•£åº¦å¤šä»»åŠ¡å­¦ä¹  (GS-MCC): è®¡ç®—å•æ¨¡æ€åˆ†ç±»æŸå¤± + KLä¸€è‡´æ€§æŸå¤±
            kl_mtl_loss = torch.tensor(0.0, device=loss.device)
            use_kl_mtl = getattr(self.config, 'use_kl_mtl', False)
            if use_kl_mtl and self.kl_criterion is not None:
                # è·å–å•æ¨¡æ€logits
                text_logits = aux_outputs.get('text_logits', None)
                audio_logits = aux_outputs.get('audio_logits', None)
                video_logits = aux_outputs.get('video_logits', None)
                
                if text_logits is not None and audio_logits is not None and video_logits is not None:
                    # è·å–æƒé‡
                    kl_weight = getattr(self.config, 'kl_mtl_weight', 1.0)
                    unimodal_weight = getattr(self.config, 'unimodal_loss_weight', 1.0)
                    
                    # 1. å•æ¨¡æ€åˆ†ç±»æŸå¤±ï¼ˆå„æ¨¡æ€ç‹¬ç«‹é¢„æµ‹çœŸå®æ ‡ç­¾ï¼‰
                    labels_for_unimodal = labels.long().squeeze()
                    unimodal_loss = (
                        self.criterion(text_logits, labels_for_unimodal) +
                        self.criterion(audio_logits, labels_for_unimodal) +
                        self.criterion(video_logits, labels_for_unimodal)
                    )
                    
                    # 2. KLä¸€è‡´æ€§æŸå¤±ï¼ˆè®©å•æ¨¡æ€é¢„æµ‹æ¥è¿‘èåˆåçš„è½¯æ ‡ç­¾ï¼‰
                    # èåˆåçš„softmaxä½œä¸ºè½¯æ ‡ç­¾ï¼ˆtargetï¼‰
                    soft_target = F.softmax(logits.detach(), dim=-1)  # [B, C]
                    
                    # å„æ¨¡æ€çš„log_softmax
                    text_log_prob = F.log_softmax(text_logits, dim=-1)
                    audio_log_prob = F.log_softmax(audio_logits, dim=-1)
                    video_log_prob = F.log_softmax(video_logits, dim=-1)
                    
                    # KLæ•£åº¦: D_KL(P || Q) where P=soft_target, Q=unimodal_prob
                    kl_loss = (
                        self.kl_criterion(text_log_prob, soft_target) +
                        self.kl_criterion(audio_log_prob, soft_target) +
                        self.kl_criterion(video_log_prob, soft_target)
                    )
                    
                    kl_mtl_loss = unimodal_weight * unimodal_loss + kl_weight * kl_loss
            
            # è®¡ç®—æ€»æŸå¤±
            total_loss = loss + self.config.sphere_loss_weight * sphere_loss + moe_loss_weight * moe_loss + mtl_lambda * mtl_loss + kl_mtl_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)
            
            self.optimizer.step()
            
            # æ›´æ–°ç»Ÿè®¡
            loss_meter.update(loss.item(), text_seq.size(0))
            sphere_loss_meter.update(sphere_loss.item(), text_seq.size(0))
            moe_loss_meter.update(moe_loss.item(), text_seq.size(0))  # â­ æ›´æ–°MoEæŸå¤±
            mtl_loss_meter.update(mtl_loss.item(), text_seq.size(0))  # â­ æ›´æ–°MTLæŸå¤±
            
            # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾
            all_preds.append(logits.detach().cpu())
            all_labels.append(labels.detach().cpu())
            
            # ====== æ··åˆå›æ”¾æ± ï¼šæ”¶é›†é«˜losså¯¹è¯ ======
            if self.use_replay_buffer and self.replay_buffer is not None:
                # è®¡ç®—å½“å‰batchçš„å¹³å‡lossï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åŠ å…¥å›æ”¾æ± ï¼‰
                batch_loss = loss.item()
                
                # åŠ¨æ€é˜ˆå€¼ï¼šä½¿ç”¨å½“å‰epochçš„å¹³å‡lossçš„å€æ•°
                dynamic_threshold = loss_meter.avg * self.replay_buffer_threshold if loss_meter.avg > 0 else self.replay_buffer_threshold
                
                if batch_loss > dynamic_threshold:
                    # å°†å½“å‰batchåŠ å…¥å›æ”¾æ± ï¼ˆå­˜å‚¨å¿…è¦ä¿¡æ¯ï¼‰
                    replay_item = {
                        'text': text_seq.detach().cpu(),
                        'audio': audio_seq.detach().cpu(),
                        'vision': video_seq.detach().cpu(),
                        'text_global': text_global.detach().cpu(),
                        'social': social.detach().cpu(),
                        'context': context.detach().cpu(),
                        'label': labels.detach().cpu(),
                        'loss': batch_loss,
                    }
                    if 'batch_dia_len' in batch:
                        replay_item['batch_dia_len'] = batch['batch_dia_len']
                    
                    self.replay_buffer.append(replay_item)
                    
                    # æ§åˆ¶å›æ”¾æ± å¤§å°ï¼ˆç§»é™¤æœ€è€çš„ï¼‰
                    if len(self.replay_buffer) > self.replay_buffer_max_size:
                        self.replay_buffer = self.replay_buffer[-self.replay_buffer_max_size:]
            # ==========================================
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆåŒ…å«MoEæŸå¤±å’ŒMTLæŸå¤±ï¼‰
            postfix_dict = {
                'loss': f'{loss_meter.avg:.4f}',
                'moe': f'{moe_loss_meter.avg:.4f}',  # â­ æ˜¾ç¤ºMoEæŸå¤±
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
            }
            if mtl_lambda > 0:
                postfix_dict['mtl'] = f'{mtl_loss_meter.avg:.4f}'  # â­ æ˜¾ç¤ºMTLæŸå¤±
            if self.use_replay_buffer and self.replay_buffer:
                postfix_dict['buf'] = f'{len(self.replay_buffer)}'  # â­ æ˜¾ç¤ºå›æ”¾æ± å¤§å°
            pbar.set_postfix(postfix_dict)
        
        # ====== æ··åˆå›æ”¾æ± ï¼šä»å›æ”¾æ± ä¸­é‡‡æ ·è®­ç»ƒ ======
        if self.use_replay_buffer and self.replay_buffer and len(self.replay_buffer) > 0:
            import random
            
            # è®¡ç®—éœ€è¦å›æ”¾çš„batchæ•°é‡
            num_replay_batches = max(1, int(len(self.train_loader) * self.replay_buffer_ratio))
            num_replay_batches = min(num_replay_batches, len(self.replay_buffer))
            
            # æŒ‰lossæ’åºï¼Œä¼˜å…ˆé€‰æ‹©é«˜lossçš„æ ·æœ¬
            sorted_buffer = sorted(self.replay_buffer, key=lambda x: x['loss'], reverse=True)
            replay_samples = sorted_buffer[:num_replay_batches]
            
            replay_loss_sum = 0.0
            replay_count = 0
            
            for replay_item in replay_samples:
                # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
                text_seq = replay_item['text'].to(self.device)
                audio_seq = replay_item['audio'].to(self.device)
                video_seq = replay_item['vision'].to(self.device)
                text_global = replay_item['text_global'].to(self.device)
                social = replay_item['social'].to(self.device)
                context = replay_item['context'].to(self.device)
                labels = replay_item['label'].to(self.device)
                
                batch_dia_len = replay_item.get('batch_dia_len', None)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                
                logits, aux_outputs = self.model(
                    text_sequence=text_seq,
                    audio_sequence=audio_seq,
                    video_sequence=video_seq,
                    text_global=text_global,
                    social_embedding=social,
                    context_embedding=context,
                    batch_dia_len=batch_dia_len,
                )
                
                # è®¡ç®—æŸå¤±
                if self.config.task_type == 'regression':
                    loss = self.criterion(logits.squeeze(-1), labels.squeeze(-1))
                else:
                    loss = self.criterion(logits, labels.long().squeeze())
                
                # æ·»åŠ è¾…åŠ©æŸå¤±
                sphere_loss = aux_outputs['sphere_loss']
                moe_loss = aux_outputs.get('moe_loss', torch.tensor(0.0, device=loss.device))
                moe_loss_weight = getattr(self.config, 'moe_loss_weight', 0.0)
                
                total_loss = loss + self.config.sphere_loss_weight * sphere_loss + moe_loss_weight * moe_loss
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                
                if self.config.grad_clip > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)
                
                self.optimizer.step()
                
                replay_loss_sum += loss.item()
                replay_count += 1
                
                # æ”¶é›†é¢„æµ‹
                all_preds.append(logits.detach().cpu())
                all_labels.append(labels.detach().cpu())
            
            if replay_count > 0:
                self.logger.info(f"  ğŸ“¦ Replay: {replay_count} batches, avg_loss={replay_loss_sum/replay_count:.4f}")
            
            # æ¸…ç†å›æ”¾æ± ä¸­å·²æ”¹å–„çš„æ ·æœ¬ï¼ˆlossé™ä½åˆ°é˜ˆå€¼ä»¥ä¸‹çš„ï¼‰
            current_avg_loss = loss_meter.avg
            self.replay_buffer = [
                item for item in self.replay_buffer 
                if item['loss'] > current_avg_loss * 0.8  # ä¿ç•™lossä»ç„¶è¾ƒé«˜çš„æ ·æœ¬
            ]
        # ==========================================
        
        # è®¡ç®—æŒ‡æ ‡
        all_preds = torch.cat(all_preds, dim=0).numpy()
        all_labels = torch.cat(all_labels, dim=0).numpy()
        
        if self.config.task_type == 'regression':
            if self.config.metrics_type == 'chsims':
                # åœ¨ç¬¬ä¸€ä¸ªepochå¯ç”¨è°ƒè¯•è¾“å‡º
                debug_mode = (epoch == 1)
                metrics = self.metrics_calc.calc_chsims_metrics(all_preds.squeeze(), all_labels.squeeze(), debug=debug_mode)
            else:
                metrics = self.metrics_calc.calc_regression_metrics(all_preds.squeeze(), all_labels.squeeze())
        else:
            pred_classes = all_preds.argmax(axis=1)
            if self.config.metrics_type == 'meld':
                # MELDä¸“ç”¨æŒ‡æ ‡ï¼šåŒ…å«æ¯ä¸ªæƒ…æ„Ÿç±»åˆ«çš„ACC/F1
                metrics = self.metrics_calc.calc_meld_metrics(pred_classes, all_labels.squeeze().astype(int))
            elif self.config.metrics_type == 'iemocap':
                # IEMOCAPä¸“ç”¨æŒ‡æ ‡ï¼š4åˆ†ç±»æƒ…æ„Ÿï¼ŒåŒ…å«æ¯ä¸ªç±»åˆ«çš„ACC/F1
                metrics = self.metrics_calc.calc_iemocap_metrics(pred_classes, all_labels.squeeze().astype(int))
            else:
                metrics = self.metrics_calc.calc_classification_metrics(pred_classes, all_labels.squeeze(), self.config.num_labels)
        
        metrics['loss'] = loss_meter.avg
        metrics['sphere_loss'] = sphere_loss_meter.avg
        metrics['moe_loss'] = moe_loss_meter.avg  # â­ è®°å½•MoEè´Ÿè½½å‡è¡¡æŸå¤±
        
        return metrics
    
    def evaluate(self, dataloader, split='valid') -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        loss_meter = AverageMeter()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            pbar = tqdm(dataloader, 
                       desc=f'{split.capitalize()} ', 
                       leave=False,
                       ncols=120,
                       bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')
            
            for batch in pbar:
                text_seq = batch['text'].to(self.device)
                audio_seq = batch['audio'].to(self.device)
                video_seq = batch['vision'].to(self.device)
                text_global = batch['text_global'].to(self.device)
                social = batch['social'].to(self.device)
                context = batch['context'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # â­ è·å– batch_dia_lenï¼š
                # - å¯¹è¯çº§ batching æ—¶ï¼šä» batch ä¸­è·å–çœŸå®çš„å¯¹è¯é•¿åº¦åˆ—è¡¨
                # - utterance çº§ batching æ—¶ï¼šæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹ï¼Œä½¿ç”¨ [1, 1, ...] ä½œä¸º fallback
                if 'batch_dia_len' in batch:
                    batch_dia_len_for_hypergraph = batch['batch_dia_len']
                elif self.config.model_config.use_hypergraph:
                    batch_dia_len_for_hypergraph = [1] * text_seq.size(0)
                else:
                    batch_dia_len_for_hypergraph = None
                
                logits, aux_outputs = self.model(
                    text_sequence=text_seq,
                    audio_sequence=audio_seq,
                    video_sequence=video_seq,
                    text_global=text_global,
                    social_embedding=social,
                    context_embedding=context,
                    batch_dia_len=batch_dia_len_for_hypergraph,
                )
                
                if self.config.task_type == 'regression':
                    loss = self.criterion(logits.squeeze(-1), labels.squeeze(-1))
                else:
                    loss = self.criterion(logits, labels.long().squeeze())
                
                loss_meter.update(loss.item(), text_seq.size(0))
                
                all_preds.append(logits.detach().cpu())
                all_labels.append(labels.detach().cpu())
        
        # è®¡ç®—æŒ‡æ ‡
        all_preds = torch.cat(all_preds, dim=0).numpy()
        all_labels = torch.cat(all_labels, dim=0).numpy()
        
        if self.config.task_type == 'regression':
            if self.config.metrics_type == 'chsims':
                # åœ¨éªŒè¯é›†ç¬¬ä¸€æ¬¡è¯„ä¼°æ—¶å¯ç”¨è°ƒè¯•è¾“å‡º
                debug_mode = (split == 'valid' and not hasattr(self, '_debug_done'))
                if debug_mode:
                    self._debug_done = True
                metrics = self.metrics_calc.calc_chsims_metrics(all_preds.squeeze(), all_labels.squeeze(), debug=debug_mode)
            else:
                metrics = self.metrics_calc.calc_regression_metrics(all_preds.squeeze(), all_labels.squeeze())
        else:
            pred_classes = all_preds.argmax(axis=1)
            if self.config.metrics_type == 'meld':
                # MELDä¸“ç”¨æŒ‡æ ‡ï¼šåŒ…å«æ¯ä¸ªæƒ…æ„Ÿç±»åˆ«çš„ACC/F1
                metrics = self.metrics_calc.calc_meld_metrics(pred_classes, all_labels.squeeze().astype(int))
            elif self.config.metrics_type == 'iemocap':
                # IEMOCAPä¸“ç”¨æŒ‡æ ‡ï¼š4åˆ†ç±»æƒ…æ„Ÿï¼ŒåŒ…å«æ¯ä¸ªç±»åˆ«çš„ACC/F1
                metrics = self.metrics_calc.calc_iemocap_metrics(pred_classes, all_labels.squeeze().astype(int))
            else:
                metrics = self.metrics_calc.calc_classification_metrics(pred_classes, all_labels.squeeze(), self.config.num_labels)
        
        metrics['loss'] = loss_meter.avg
        
        return metrics
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("Starting training...")
        
        for epoch in range(1, self.config.num_epochs + 1):
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            
            # éªŒè¯
            valid_metrics = self.evaluate(self.valid_loader, 'valid')
            
            # æ—¥å¿—
            self.logger.info(f"\nEpoch {epoch}/{self.config.num_epochs}")
            self.logger.info(f"Train: {dict_to_str(train_metrics)}")
            self.logger.info(f"Valid: {dict_to_str(valid_metrics)}")
            
            # æ¯ä¸ªepochåè¯„ä¼°æµ‹è¯•é›†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            test_metrics_epoch = None
            if getattr(self.config, 'eval_test_every_epoch', False):
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"Epoch {epoch} - æµ‹è¯•é›†è¯„ä¼°ï¼ˆä»…ç›‘æ§ï¼Œä¸å½±å“æ—©åœï¼‰")
                self.logger.info(f"{'='*60}")
                test_metrics_epoch = self.evaluate(self.test_loader, split='test')
                self.logger.info(f"Test: {dict_to_str(test_metrics_epoch)}")
                self.logger.info(f"{'='*60}\n")
            
            # è®°å½•æŒ‡æ ‡åˆ°txtæ–‡ä»¶
            self.log_metrics_to_file(epoch, train_metrics, valid_metrics, test_metrics_epoch)
            
            # è®°å½•æŒ‡æ ‡åˆ°å†å²ï¼ˆç”¨äºç»˜å›¾ï¼‰
            self.metrics_history.update('train', epoch, train_metrics)
            self.metrics_history.update('valid', epoch, valid_metrics)
            if test_metrics_epoch is not None:
                self.metrics_history.update('test', epoch, test_metrics_epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                if self.config.scheduler_type == 'reduce_on_plateau':
                    monitor_metric = valid_metrics.get('mae', valid_metrics.get('loss'))
                    self.scheduler.step(monitor_metric)
                else:
                    self.scheduler.step()
            
            # æ—©åœæ£€æŸ¥
            # ä½¿ç”¨é…ç½®æŒ‡å®šçš„æŒ‡æ ‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°é»˜è®¤
            metric_name = getattr(self.config, 'early_stop_metric', 'mae')
            
            # â­ æ–°å¢ï¼šæ”¯æŒç»¼åˆæŒ‡æ ‡
            if metric_name == 'composite':
                # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆå½’ä¸€åŒ–ååŠ æƒï¼‰
                mae_score = -valid_metrics.get('MAE', 1.0)  # è´Ÿå€¼ï¼Œå› ä¸ºè¶Šå°è¶Šå¥½
                corr_score = valid_metrics.get('Corr', 0.0)
                acc5_score = valid_metrics.get('Acc_5', 0.0)
                
                monitor_metric = (
                    self.config.composite_mae_weight * mae_score +
                    self.config.composite_corr_weight * corr_score +
                    self.config.composite_acc5_weight * acc5_score
                )
                
                self.logger.info(f"  Composite Score: {monitor_metric:.4f} "
                               f"(MAE={mae_score:.3f}, Corr={corr_score:.3f}, Acc5={acc5_score:.3f})")
            else:
                # å¤„ç†å¤§å°å†™ä¸åŒ¹é…ï¼šutils.pyè¿”å›çš„é”®æ˜¯Acc_2, F1_2ç­‰ï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
                metric_name_variants = [
                    metric_name,  # åŸå§‹ï¼šf1_weighted
                    metric_name.upper(),  # å…¨å¤§å†™ï¼šF1_WEIGHTED
                    'Acc_2' if metric_name == 'acc_2' else metric_name,
                    'Acc_3' if metric_name == 'acc_3' else metric_name,
                    'F1_2' if metric_name == 'f1_2' else metric_name,
                    'F1_3' if metric_name == 'f1_3' else metric_name,
                    'F1_5' if metric_name == 'f1_5' else metric_name,
                    'Acc_5' if metric_name == 'acc_5' else metric_name,
                    'MAE' if metric_name == 'mae' else metric_name,
                    'Corr' if metric_name == 'corr' else metric_name,
                    'F1_weighted' if metric_name == 'f1_weighted' else metric_name,  # åˆ†ç±»ä»»åŠ¡
                    'F1_macro' if metric_name == 'f1_macro' else metric_name,
                    'F1_micro' if metric_name == 'f1_micro' else metric_name,
                    'Acc' if metric_name == 'acc' else metric_name,
                ]
                monitor_metric = None
                for variant in metric_name_variants:
                    if variant in valid_metrics:
                        monitor_metric = valid_metrics[variant]
                        break
                if monitor_metric is None:
                    monitor_metric = valid_metrics.get('MAE', valid_metrics.get('loss', 0.0))
            
            if self.config.metric_mode == 'min':
                is_best = monitor_metric < self.best_metric
            else:
                is_best = monitor_metric > self.best_metric
            
            if is_best:
                self.best_metric = monitor_metric
                self.best_epoch = epoch
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_path = os.path.join(self.config.save_dir, 'best_model.pth')
                save_checkpoint(
                    model=self.model,
                    optimizer=self.optimizer,
                    scheduler=self.scheduler,
                    epoch=epoch,
                    metrics=valid_metrics,
                    save_path=save_path,
                    is_best=True,
                )
                
                self.logger.info(f"âœ“ Best model saved! (metric: {self.best_metric:.4f})")
            
            # æ—©åœ
            self.early_stopping(monitor_metric, epoch)
            if self.early_stopping.early_stop:
                self.logger.info(f"Early stopping triggered at epoch {epoch}")
                break
        
        # æµ‹è¯•
        self.logger.info("\nEvaluating on test set...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint = torch.load(
            os.path.join(self.config.save_dir, 'best_model.pth'),
            map_location=self.device,
            weights_only=False  # PyTorch 2.6+ éœ€è¦æ­¤å‚æ•°
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        test_metrics = self.evaluate(self.test_loader, 'test')
        self.logger.info(f"Test: {dict_to_str(test_metrics)}")
        
        self.logger.info(f"\nTraining completed! Best epoch: {self.best_epoch}")
        self.logger.info(f"Best validation metric: {self.best_metric:.4f}")
        
        # ====== è®­ç»ƒç»“æŸåç»˜åˆ¶æ›²çº¿å›¾ ======
        if self.plotting_enabled and self.plotter is not None:
            self.logger.info("\n" + "="*60)
            self.logger.info("ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾...")
            self.logger.info("="*60)
            
            # åˆ¤æ–­æ˜¯å¦åŒ…å«æµ‹è¯•é›†æ•°æ®
            include_test = getattr(self.config, 'eval_test_every_epoch', False)
            
            # ç»˜åˆ¶å„ä¸ªæŒ‡æ ‡çš„ç‹¬ç«‹å›¾
            saved_paths = self.plotter.plot_all_metrics(
                self.metrics_history, 
                self.plot_config,
                include_test=include_test
            )
            
            # ç»˜åˆ¶ç»„åˆå›¾
            combined_path = self.plotter.plot_combined_figure(
                self.metrics_history,
                self.plot_config,
                include_test=include_test
            )
            
            # ä¿å­˜æŒ‡æ ‡å†å²åˆ°JSON
            history_path = os.path.join(self.config.save_dir, 'metrics_history.json')
            self.metrics_history.save_to_json(history_path)
            self.logger.info(f"âœ“ å·²ä¿å­˜æŒ‡æ ‡å†å²: {history_path}")
            
            self.logger.info("="*60 + "\n")
        # ==========================================


def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€è®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ')
    parser.add_argument('--dataset', type=str, required=True,
                        choices=['chsims', 'chsimsv2', 'meld', 'iemocap'],
                        help='æ•°æ®é›†åç§°')
    parser.add_argument('--batch_size', type=int, default=None,
                        help='æ‰¹å¤§å°ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼ï¼‰')
    parser.add_argument('--learning_rate', type=float, default=None,
                        help='å­¦ä¹ ç‡ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼ï¼‰')
    parser.add_argument('--num_epochs', type=int, default=None,
                        help='è®­ç»ƒè½®æ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼ï¼‰')
    parser.add_argument('--data_dir', type=str, default=None,
                        help='æ•°æ®é›†ç›®å½•è·¯å¾„ï¼ˆè¦†ç›–é»˜è®¤è·¯å¾„ï¼‰')
    parser.add_argument('--seq_length', type=int, default=None,
                        help='åºåˆ—é•¿åº¦/æœ€å¤§å¸§æ•°ï¼ˆå»ºè®®ï¼šCH-SIMS/v2=70, MELD=80, IEMOCAP=110ï¼‰')
    parser.add_argument('--early_stop_patience', type=int, default=None,
                        help='æ—©åœç­‰å¾…è½®æ•°ï¼ˆå¯é€‰ï¼Œ0=ç¦ç”¨æ—©åœï¼‰')
    parser.add_argument('--early_stop_metric', type=str, default=None,
                        choices=['mae', 'loss', 'acc_2', 'acc_3', 'acc_5', 'f1_2', 'f1_3', 'f1_5', 'corr', 'composite'],
                        help='æ—©åœç›‘æ§æŒ‡æ ‡ï¼ˆcompositeä¸ºç»¼åˆæŒ‡æ ‡ï¼š0.4*MAE + 0.3*Corr + 0.3*Acc5ï¼‰')
    parser.add_argument('--sphere_loss_weight', type=float, default=None,
                        help='è¶…çƒä½“æŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.01ï¼‰')
    parser.add_argument('--moe_loss_weight', type=float, default=None,
                        help='MoEè´Ÿè½½å‡è¡¡æŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.01ï¼Œé˜²æ­¢ä¸“å®¶åç¼©ï¼‰')
    
    # CH-SIMSv2 MTLå‚æ•°
    parser.add_argument('--chsimsv2_mtl_lambda', type=float, default=None,
                        help='CH-SIMSv2å¤šä»»åŠ¡å­¦ä¹ çš„è¾…åŠ©æŸå¤±æƒé‡ï¼ˆå•æ¨¡æ€æ ‡ç­¾ï¼‰ï¼Œ0.0=å…³é—­')
    
    # æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument('--loss_function', type=str, default=None,
                        choices=['mse', 'l1', 'mae', 'focal_mse', 'huber', 'focal', 'ce'],
                        help='æŸå¤±å‡½æ•°ç±»å‹')
    parser.add_argument('--focal_gamma', type=float, default=None,
                        help='Focal Loss gammaå‚æ•°ï¼ˆåˆå§‹å€¼ï¼Œå¦‚å¯ç”¨åŠ¨æ€gammaï¼‰')
    parser.add_argument('--focal_dynamic_gamma', action='store_true',
                        help='å¯ç”¨åŠ¨æ€gammaè¡°å‡ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­gammaé€æ¸é™ä½ï¼‰')
    parser.add_argument('--focal_gamma_min', type=float, default=0.5,
                        help='åŠ¨æ€gammaçš„æœ€å°å€¼ï¼ˆé»˜è®¤0.5ï¼‰')
    parser.add_argument('--focal_gamma_decay_mode', type=str, default='cosine',
                        choices=['linear', 'exponential', 'cosine', 'step'],
                        help='Gammaè¡°å‡æ¨¡å¼ï¼ˆé»˜è®¤cosineï¼‰')
    parser.add_argument('--use_class_weights', action='store_true',
                        help='å¯ç”¨ç±»åˆ«æƒé‡')
    parser.add_argument('--no_class_weights', action='store_true',
                        help='ç¦ç”¨ç±»åˆ«æƒé‡')
    parser.add_argument('--label_smoothing', type=float, default=None,
                        help='Label Smoothingç³»æ•° (é»˜è®¤0.0)')

    # é‡‡æ ·å‚æ•°
    parser.add_argument('--use_weighted_sampler', action='store_true',
                        help='å¯ç”¨WeightedRandomSamplerè¿›è¡Œç±»åˆ«å¹³è¡¡é‡‡æ ·')
    parser.add_argument('--no_weighted_sampler', action='store_true',
                        help='ç¦ç”¨WeightedRandomSampler')

    # â­ è¯¾ç¨‹å­¦ä¹ å‚æ•°
    parser.add_argument('--curriculum_mode', type=str, default=None,
                        choices=['none', 'freeze_backbone', 'alpha_blending'],
                        help='è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ï¼šnone=å…³é—­ï¼Œfreeze_backbone=å†»ç»“éª¨å¹²ç½‘ç»œï¼Œalpha_blending=æ¸è¿›å¼MoEæ··åˆ')
    parser.add_argument('--curriculum_epochs', type=int, default=None,
                        help='è¯¾ç¨‹å­¦ä¹ æŒç»­çš„Epochæ•°ï¼ˆé»˜è®¤ï¼š5ï¼‰')
    
    parser.add_argument('--hidden_dim', type=int, default=None,
                        help='éšè—å±‚ç»´åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤256ï¼‰')
    parser.add_argument('--dropout_p', type=float, default=None,
                        help='Dropoutç‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.1ï¼‰')
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    parser.add_argument('--scheduler_type', type=str, default=None,
                        choices=['step', 'cosine', 'reduce_on_plateau', 'none'],
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤cosineï¼‰')
    parser.add_argument('--scheduler_gamma', type=float, default=None,
                        help='å­¦ä¹ ç‡è¡°å‡ç³»æ•°ï¼ˆé»˜è®¤0.5ï¼‰')
    parser.add_argument('--scheduler_patience', type=int, default=None,
                        help='ç­‰å¾…è½®æ•°ï¼ˆé»˜è®¤5ï¼Œä»…ç”¨äºreduce_on_plateauï¼‰')
    parser.add_argument('--scheduler_step_size', type=int, default=None,
                        help='æ­¥é•¿ï¼ˆé»˜è®¤10ï¼Œä»…ç”¨äºstepï¼‰')
    parser.add_argument('--warmup_ratio', type=float, default=None,
                        help='å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹ï¼ˆé»˜è®¤0.0å…³é—­ï¼Œ0.1=å‰10%%æ­¥æ•°ç”¨äºé¢„çƒ­ï¼‰')
    
    # ç»„ä»¶å‚æ•°
    parser.add_argument('--n_key_frames', type=int, default=None,
                        help='å…³é”®å¸§æ•°é‡ï¼ˆå¯é€‰ï¼Œæ—§ç‰ˆå‚æ•°ï¼Œä¼˜å…ˆçº§ä½äºn_segments/frame_ratioï¼‰')
    parser.add_argument('--key_frame_segment_size', type=int, default=None,
                        help='å…³é”®å¸§åˆ†æ®µå¤§å°ï¼ˆå¯é€‰ï¼Œæ—§ç‰ˆå‚æ•°ï¼Œä¼˜å…ˆçº§ä½äºn_segments/frame_ratioï¼‰')
    # æ–°çš„ç™¾åˆ†æ¯”æ¨¡å¼å‚æ•°
    parser.add_argument('--n_segments', type=int, default=None,
                        help='MDP3åˆ†æ®µæ•°ï¼ˆé»˜è®¤ï¼šconfig.model_config.n_segmentsï¼‰')
    parser.add_argument('--frame_ratio', type=int, default=None,
                        help='æ¯æ®µé€‰æ‹©çš„å¸§ç™¾åˆ†æ¯”ï¼Œ1-100ï¼ˆé»˜è®¤ï¼šconfig.model_config.frame_ratioï¼‰')
    parser.add_argument('--num_film_experts', type=int, default=None,
                        help='MoE-FiLMä¸“å®¶æ•°é‡ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--film_top_k', type=int, default=None,
                        help='FiLM Top-Ké€‰æ‹©ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--num_hypergraph_layers', type=int, default=None,
                        help='è¶…å›¾å±‚æ•°ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--hypergraph_use_residue', action='store_true',
                        help='å¯ç”¨è¶…å›¾æ‹¼æ¥æ®‹å·®ï¼ˆM3NETåŸå§‹æ–¹å¼ï¼‰')
    parser.add_argument('--hypergraph_no_residue', action='store_true',
                        help='ç¦ç”¨è¶…å›¾æ‹¼æ¥æ®‹å·®')
    parser.add_argument('--num_fourier_layers', type=int, default=None,
                        help='å‚…é‡Œå¶å±‚æ•°ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--fgn_use_residue', action='store_true',
                        help='å¯ç”¨FGNæ‹¼æ¥æ®‹å·®ï¼ˆGS-MCCåŸå§‹æ–¹å¼ï¼‰')
    parser.add_argument('--fgn_no_residue', action='store_true',
                        help='ç¦ç”¨FGNæ‹¼æ¥æ®‹å·®')
    parser.add_argument('--fourier_sparsity_threshold', type=float, default=None,
                        help='å‚…é‡Œå¶ç¨€ç–é˜ˆå€¼ï¼ˆé»˜è®¤0.01ï¼‰')
    parser.add_argument('--fourier_hidden_size_factor', type=int, default=None,
                        help='å‚…é‡Œå¶éšè—å±‚å€æ•°ï¼ˆé»˜è®¤1ï¼‰')
    
    # æ··åˆå›æ”¾æ± å‚æ•° â­ æ–°å¢
    parser.add_argument('--use_replay_buffer', action='store_true',
                        help='å¯ç”¨æ··åˆå›æ”¾æ± ï¼ˆExperience Replayï¼‰')
    parser.add_argument('--no_replay_buffer', action='store_true',
                        help='ç¦ç”¨æ··åˆå›æ”¾æ± ')
    parser.add_argument('--replay_buffer_threshold', type=float, default=1.5,
                        help='å›æ”¾æ± Lossé˜ˆå€¼å€æ•°ï¼ˆé»˜è®¤1.5ï¼Œå³avg_loss*1.5ï¼‰')
    parser.add_argument('--replay_buffer_ratio', type=float, default=0.2,
                        help='å›æ”¾è®­ç»ƒæ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼Œå³é¢å¤–è®­ç»ƒ20%%çš„batchï¼‰')
    parser.add_argument('--replay_buffer_max_size', type=int, default=500,
                        help='å›æ”¾æ± æœ€å¤§å®¹é‡ï¼ˆé»˜è®¤500ï¼‰')
    
    # æ¨¡æ€å’Œå…³é”®å¸§åˆ†æå‚æ•° â­ æ–°å¢
    parser.add_argument('--enable_modality_analysis', action='store_true',
                        help='å¯ç”¨æ¨¡æ€è´¡çŒ®åº¦åˆ†æï¼ˆä¼šå½±å“è®­ç»ƒé€Ÿåº¦ï¼‰')
    parser.add_argument('--analyze_modality_every', type=int, default=None,
                        help='æ¯Nä¸ªbatchè¿›è¡Œæ¨¡æ€åˆ†æï¼ˆé»˜è®¤ï¼š10ï¼Œé€‚åº”å°æ•°æ®é›†ï¼‰')
    parser.add_argument('--keyframe_log_every', type=int, default=None,
                        help='æ¯Nä¸ªutteranceæ‰“å°å…³é”®å¸§ç»Ÿè®¡ï¼ˆé»˜è®¤ï¼š32ï¼‰')
    parser.add_argument('--modality_analysis_epochs', type=int, default=None,
                        help='åœ¨å‰Nä¸ªepochè¿›è¡Œæ¨¡æ€åˆ†æï¼ˆé»˜è®¤ï¼š3ï¼‰')
    parser.add_argument('--enable_keyframe_logging', action='store_true',
                        help='å¯ç”¨å…³é”®å¸§ç»Ÿè®¡æ‰“å°')
    parser.add_argument('--eval_test_every_epoch', action='store_true',
                        help='æ¯ä¸ªepochååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆä»…ç›‘æ§ï¼Œä¸å½±å“æ—©åœï¼‰')
    
    # ç»„ä»¶å¼€å…³ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰
    parser.add_argument('--no_key_frame_selector', action='store_true',
                        help='ç¦ç”¨å…³é”®å¸§é€‰æ‹©')
    parser.add_argument('--no_coupled_mamba', action='store_true',
                        help='ç¦ç”¨Coupled Mambaï¼ˆä½¿ç”¨ç‹¬ç«‹Mambaï¼‰')
    parser.add_argument('--no_moe_film', action='store_true',
                        help='ç¦ç”¨MoE-FiLMè°ƒåˆ¶')
    parser.add_argument('--no_hypergraph', action='store_true',
                        help='ç¦ç”¨è¶…å›¾å»ºæ¨¡')
    parser.add_argument('--no_frequency_decomp', action='store_true',
                        help='ç¦ç”¨é¢‘åŸŸåˆ†è§£')
    parser.add_argument('--no_sphere_reg', action='store_true',
                        help='ç¦ç”¨è¶…çƒä½“æ­£åˆ™åŒ–')
    
    # ç»„ä»¶å¼€å…³ï¼ˆå¼€å¯ç±»ï¼‰- ç”¨äºå¼€å¯é»˜è®¤å…³é—­çš„ç»„ä»¶
    parser.add_argument('--use_dsps', action='store_true',
                        help='å¯ç”¨DSPSæ¡ä»¶åŒ–SSMï¼ˆåœ¨Mambaçš„dt/B/Cè·¯å¾„æ³¨å…¥æ¡ä»¶ï¼‰âš ï¸ ä¸MoE-FiLMäº’æ–¥')
    parser.add_argument('--dsps_strength', type=float, default=0.1,
                        help='DSPSå¼ºåº¦å› å­ï¼ˆ0.0=æ— æ•ˆæœï¼Œ1.0=å®Œå…¨æ•ˆæœï¼Œé»˜è®¤0.1ï¼‰')
    parser.add_argument('--use_hypergraph', action='store_true',
                        help='å¯ç”¨è¶…å›¾å»ºæ¨¡ï¼ˆé»˜è®¤å…³é—­ï¼‰âš ï¸ è‡ªåŠ¨å¯ç”¨å¯¹è¯çº§batching')
    parser.add_argument('--use_frequency_decomp', action='store_true',
                        help='å¯ç”¨é¢‘åŸŸåˆ†è§£ï¼ˆé»˜è®¤å…³é—­ï¼‰')
    
    # â­ KLæ•£åº¦å¤šä»»åŠ¡å­¦ä¹  (GS-MCC)
    parser.add_argument('--use_kl_mtl', action='store_true',
                        help='å¯ç”¨KLæ•£åº¦å¤šä»»åŠ¡å­¦ä¹ ï¼ˆè®©å•æ¨¡æ€é¢„æµ‹æ¥è¿‘èåˆé¢„æµ‹ï¼‰')
    parser.add_argument('--kl_mtl_weight', type=float, default=1.0,
                        help='KLæ•£åº¦æŸå¤±çš„æƒé‡ï¼ˆé»˜è®¤1.0ï¼‰')
    parser.add_argument('--unimodal_loss_weight', type=float, default=1.0,
                        help='å•æ¨¡æ€åˆ†ç±»æŸå¤±çš„æƒé‡ï¼ˆé»˜è®¤1.0ï¼‰')
    parser.add_argument('--use_sphere_reg', action='store_true',
                        help='å¯ç”¨è¶…çƒä½“æ­£åˆ™åŒ–ï¼ˆé»˜è®¤å…³é—­ï¼‰')
    
    # å¯¹è¯çº§ batching å‚æ•°ï¼ˆè¶…å›¾å»ºæ¨¡éœ€è¦ï¼‰
    parser.add_argument('--dialogue_batch_size', type=int, default=8,
                        help='[å·²å¼ƒç”¨] ä½¿ç”¨ --max_utterances_per_batch ä»£æ›¿')
    parser.add_argument('--max_dialogue_len', type=int, default=50,
                        help='å•ä¸ªå¯¹è¯æœ€å¤§utteranceæ•°é‡ï¼ˆé»˜è®¤50ï¼Œè¶…è¿‡çš„å¯¹è¯è¢«è·³è¿‡ï¼‰')
    parser.add_argument('--max_utterances_per_batch', type=int, default=128,
                        help='æ¯æ‰¹æœ€å¤§utteranceæ•°ï¼ˆæ§åˆ¶æ˜¾å­˜ï¼Œé»˜è®¤128ï¼‰')
    parser.add_argument('--no_direct_fusion_priors', action='store_true',
                        help='ç¦æ­¢social/contextç›´æ¥å‚ä¸èåˆï¼ˆåªç”¨äºFiLMè°ƒåˆ¶ï¼‰')
    parser.add_argument('--use_improved_mlp', action='store_true',
                        help='ä½¿ç”¨æ”¹è¿›ç‰ˆMLPï¼ˆ4å±‚æ·±å±‚+GELU+æ®‹å·®+LayerNormï¼‰')
    parser.add_argument('--mlp_dropout', type=float, default=0.2,
                        help='æ”¹è¿›ç‰ˆMLPçš„Dropoutæ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼‰')
    parser.add_argument('--mlp_expansion_ratio', type=int, default=4,
                        help='æ”¹è¿›ç‰ˆMLPä¸­é—´å±‚æ‰©ç»´å€æ•°ï¼ˆé»˜è®¤4ï¼‰')
    
    # æŒ‡æ ‡è®°å½•æ–‡ä»¶
    parser.add_argument('--metrics_file', type=str, default=None,
                        help='æŒ‡æ ‡è®°å½•æ–‡ä»¶è·¯å¾„ï¼ˆtxtæ–‡ä»¶ï¼‰')
    
    # å¤šGPUæ”¯æŒï¼šè‡ªå®šä¹‰ä¿å­˜ç›®å½•
    parser.add_argument('--save_dir', type=str, default=None,
                        help='æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆç”¨äºå¤šGPUå¹¶è¡Œè®­ç»ƒé¿å…å†²çªï¼‰')
    parser.add_argument('--log_dir', type=str, default=None,
                        help='æ—¥å¿—ç›®å½•ï¼ˆç”¨äºå¤šGPUå¹¶è¡Œè®­ç»ƒé¿å…å†²çªï¼‰')
    
    # ====== è®­ç»ƒæ›²çº¿ç»˜å›¾å¼€å…³ï¼ˆç®€åŒ–ç‰ˆï¼‰======
    parser.add_argument('--enable_plotting', action='store_true',
                        help='å¯ç”¨è®­ç»ƒæ›²çº¿ç»˜å›¾ï¼ˆè‡ªåŠ¨æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©åˆé€‚çš„æŒ‡æ ‡ï¼‰')
    parser.add_argument('--disable_plotting', action='store_true',
                        help='ç¦ç”¨è®­ç»ƒæ›²çº¿ç»˜å›¾')
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    config = get_config(args.dataset)
    
    # è¦†ç›–åŸºç¡€å‚æ•°
    if args.data_dir is not None:
        config.data_dir = args.data_dir
    if args.seq_length is not None:
        config.seq_length = args.seq_length
    if args.batch_size is not None:
        config.batch_size = args.batch_size
    if args.learning_rate is not None:
        config.learning_rate = args.learning_rate
    if args.num_epochs is not None:
        config.num_epochs = args.num_epochs
    if args.early_stop_patience is not None:
        config.early_stop_patience = args.early_stop_patience
    if args.early_stop_metric is not None:
        config.early_stop_metric = args.early_stop_metric
    if args.sphere_loss_weight is not None:
        config.sphere_loss_weight = args.sphere_loss_weight
    if args.moe_loss_weight is not None:
        config.moe_loss_weight = args.moe_loss_weight
    
    # CH-SIMSv2 MTLå‚æ•°
    if args.chsimsv2_mtl_lambda is not None:
        if hasattr(config, 'chsimsv2_mtl_lambda'):
            config.chsimsv2_mtl_lambda = args.chsimsv2_mtl_lambda

    # æŸå¤±å‡½æ•°å‚æ•°è¦†ç›–
    if args.loss_function is not None:
        config.loss_function = args.loss_function
    if args.focal_gamma is not None:
        config.focal_gamma = args.focal_gamma
    if args.focal_dynamic_gamma:
        config.focal_dynamic_gamma = True
        config.focal_gamma_min = args.focal_gamma_min
        config.focal_gamma_decay_mode = args.focal_gamma_decay_mode
    if args.use_class_weights:
        config.use_class_weights = True
    if args.no_class_weights:
        config.use_class_weights = False
    if args.label_smoothing is not None:
        config.label_smoothing = args.label_smoothing
    
    # é‡‡æ ·å‚æ•°
    if args.use_weighted_sampler:
        config.use_weighted_sampler = True
    if args.no_weighted_sampler:
        config.use_weighted_sampler = False
    
    # â­ è¯¾ç¨‹å­¦ä¹ å‚æ•°è¦†ç›–
    if args.curriculum_mode is not None:
        config.curriculum_mode = args.curriculum_mode
    if args.curriculum_epochs is not None:
        config.curriculum_epochs = args.curriculum_epochs
    
    if args.hidden_dim is not None:
        config.hidden_dim = args.hidden_dim
    if args.dropout_p is not None:
        config.dropout_p = args.dropout_p
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    if args.scheduler_type is not None:
        config.scheduler_type = args.scheduler_type
    if args.scheduler_gamma is not None:
        config.scheduler_gamma = args.scheduler_gamma
    if args.scheduler_patience is not None:
        config.scheduler_patience = args.scheduler_patience
    if args.scheduler_step_size is not None:
        config.scheduler_step_size = args.scheduler_step_size
    if args.warmup_ratio is not None:
        config.warmup_ratio = args.warmup_ratio
    
    # æ¨¡æ€å’Œå…³é”®å¸§åˆ†æå‚æ•°
    if args.enable_modality_analysis:
        config.enable_modality_analysis = True
    if args.analyze_modality_every is not None:
        config.analyze_modality_every = args.analyze_modality_every
    if args.keyframe_log_every is not None:
        config.keyframe_log_every = args.keyframe_log_every
    if args.modality_analysis_epochs is not None:
        config.modality_analysis_epochs = args.modality_analysis_epochs
    if args.enable_keyframe_logging:
        config.enable_keyframe_logging = True
    if args.eval_test_every_epoch:
        config.eval_test_every_epoch = True
    
    # åº”ç”¨ç»„ä»¶å‚æ•°
    if args.n_segments is not None:
        config.model_config.n_segments = max(1, args.n_segments)
    if args.frame_ratio is not None:
        if not (1 <= args.frame_ratio <= 100):
            raise ValueError("--frame_ratio å¿…é¡»åœ¨1åˆ°100ä¹‹é—´")
        config.model_config.frame_ratio = args.frame_ratio
    # å…¼å®¹æ—§å‚æ•°ï¼šè‹¥ç”¨æˆ·ä»ç„¶ä½¿ç”¨n_key_frames/key_frame_segment_sizeï¼Œåˆ™å›é€€åˆ°å›ºå®šå¸§æ•°æ¨¡å¼
    if args.n_key_frames is not None:
        config.model_config.n_key_frames = args.n_key_frames
    if args.key_frame_segment_size is not None:
        config.model_config.key_frame_segment_size = args.key_frame_segment_size
    if args.num_film_experts is not None:
        config.model_config.num_film_experts = args.num_film_experts
    if args.film_top_k is not None:
        config.model_config.film_top_k = args.film_top_k
    if args.num_hypergraph_layers is not None:
        config.model_config.num_hypergraph_layers = args.num_hypergraph_layers
    if args.hypergraph_use_residue:
        config.model_config.hypergraph_use_residue = True
    if args.hypergraph_no_residue:
        config.model_config.hypergraph_use_residue = False
    if args.num_fourier_layers is not None:
        config.model_config.num_fourier_layers = args.num_fourier_layers
    if args.fgn_use_residue:
        config.model_config.fgn_use_residue = True
    if args.fgn_no_residue:
        config.model_config.fgn_use_residue = False
    if args.fourier_sparsity_threshold is not None:
        config.model_config.fourier_sparsity_threshold = args.fourier_sparsity_threshold
    if args.fourier_hidden_size_factor is not None:
        config.model_config.fourier_hidden_size_factor = args.fourier_hidden_size_factor
    
    # åº”ç”¨æ··åˆå›æ”¾æ± å‚æ•°
    if args.use_replay_buffer:
        config.use_replay_buffer = True
    if args.no_replay_buffer:
        config.use_replay_buffer = False
    if args.replay_buffer_threshold is not None:
        config.replay_buffer_threshold = args.replay_buffer_threshold
    if args.replay_buffer_ratio is not None:
        config.replay_buffer_ratio = args.replay_buffer_ratio
    if args.replay_buffer_max_size is not None:
        config.replay_buffer_max_size = args.replay_buffer_max_size
    
    # åº”ç”¨ç»„ä»¶å¼€å…³ï¼ˆå…³é—­ç±»ï¼‰
    if args.no_key_frame_selector:
        config.model_config.use_key_frame_selector = False
    if args.no_coupled_mamba:
        config.model_config.use_coupled_mamba = False
    if args.no_moe_film:
        config.model_config.use_moe_film = False
    if args.no_hypergraph:
        config.model_config.use_hypergraph = False
    if args.no_frequency_decomp:
        config.model_config.use_frequency_decomp = False
    if args.no_sphere_reg:
        config.model_config.use_sphere_regularization = False
    if args.no_direct_fusion_priors:
        config.model_config.direct_fusion_priors = False
    
    # åº”ç”¨ç»„ä»¶å¼€å…³ï¼ˆå¼€å¯ç±»ï¼‰- ç”¨äºå¼€å¯é»˜è®¤å…³é—­çš„ç»„ä»¶
    if args.use_dsps:
        config.model_config.use_dsps = True
    if hasattr(args, 'dsps_strength') and args.dsps_strength is not None:
        config.model_config.dsps_strength = args.dsps_strength
    
    # â­ KLæ•£åº¦å¤šä»»åŠ¡å­¦ä¹ å‚æ•°
    if args.use_kl_mtl:
        config.model_config.use_kl_mtl = True
        config.use_kl_mtl = True  # ä¹Ÿè®¾ç½®åˆ°configçº§åˆ«
    if hasattr(args, 'kl_mtl_weight') and args.kl_mtl_weight is not None:
        config.model_config.kl_mtl_weight = args.kl_mtl_weight
        config.kl_mtl_weight = args.kl_mtl_weight
    if hasattr(args, 'unimodal_loss_weight') and args.unimodal_loss_weight is not None:
        config.model_config.unimodal_loss_weight = args.unimodal_loss_weight
        config.unimodal_loss_weight = args.unimodal_loss_weight
    
    if args.use_hypergraph:
        config.model_config.use_hypergraph = True
        # â­ è¶…å›¾å»ºæ¨¡éœ€è¦å¯¹è¯çº§ batching
        config.use_dialogue_batching = True
        print("âš ï¸  å¯ç”¨è¶…å›¾å»ºæ¨¡ï¼Œè‡ªåŠ¨å¯ç”¨å¯¹è¯çº§ batching")
    if args.use_frequency_decomp:
        config.model_config.use_frequency_decomp = True
    if args.use_sphere_reg:
        config.model_config.use_sphere_regularization = True
    if args.use_improved_mlp:
        config.model_config.use_improved_mlp = True
    
    # å¯¹è¯çº§ batching å‚æ•°
    if hasattr(args, 'dialogue_batch_size') and args.dialogue_batch_size:
        config.dialogue_batch_size = args.dialogue_batch_size
    if hasattr(args, 'max_dialogue_len') and args.max_dialogue_len:
        config.model_config.max_dialogue_len = args.max_dialogue_len
    if hasattr(args, 'max_utterances_per_batch') and args.max_utterances_per_batch:
        config.max_utterances_per_batch = args.max_utterances_per_batch
    if args.mlp_dropout is not None:
        config.model_config.mlp_dropout = args.mlp_dropout
    if args.mlp_expansion_ratio is not None:
        config.model_config.mlp_expansion_ratio = args.mlp_expansion_ratio
    
    # æŒ‡æ ‡è®°å½•æ–‡ä»¶
    if args.metrics_file is not None:
        config.metrics_file = args.metrics_file
    
    # å¤šGPUæ”¯æŒï¼šè‡ªå®šä¹‰ä¿å­˜ç›®å½•å’Œæ—¥å¿—ç›®å½•
    if args.save_dir is not None:
        config.save_dir = args.save_dir
    if args.log_dir is not None:
        config.log_dir = args.log_dir
    
    # ====== ç»˜å›¾å¼€å…³ï¼ˆè‡ªåŠ¨æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æŒ‡æ ‡ï¼‰======
    if args.disable_plotting:
        # å¼ºåˆ¶ç¦ç”¨ç»˜å›¾
        config.plotting_enabled = False
    elif args.enable_plotting:
        # å¯ç”¨ç»˜å›¾ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æŒ‡æ ‡
        config.plotting_enabled = True
        
        if config.task_type == 'regression':
            # å›å½’ä»»åŠ¡ (CH-SIMS, CH-SIMSv2): MAE, Loss, Corr, Acc-2/3/5
            config.plot_mae = True
            config.plot_loss = True
            config.plot_corr = True
            config.plot_acc2 = True
            config.plot_acc3 = True
            config.plot_acc5 = True
            # åˆ†ç±»æŒ‡æ ‡ä¸é€‚ç”¨
            config.plot_acc = False
            config.plot_f1_weighted = False
            config.plot_f1_macro = False
        else:
            # åˆ†ç±»ä»»åŠ¡ (MELD, IEMOCAP): Loss, Acc, F1_weighted, F1_macro
            config.plot_loss = True
            config.plot_acc = True  # æ•´ä½“å‡†ç¡®ç‡
            config.plot_f1_weighted = True
            config.plot_f1_macro = True
            # å›å½’æŒ‡æ ‡ä¸é€‚ç”¨
            config.plot_mae = False
            config.plot_corr = False
            config.plot_acc2 = False
            config.plot_acc3 = False
            config.plot_acc5 = False
    else:
        # é»˜è®¤ç¦ç”¨ç»˜å›¾
        config.plotting_enabled = False
    
    # ========================================
    # â­ MoE-FiLM ä¸ DSPS äº’æ–¥æ ¡éªŒ
    # ========================================
    # è¿™ä¸¤ç§è°ƒåˆ¶æ–¹å¼ä¸èƒ½åŒæ—¶å¯ç”¨ï¼š
    # - MoE-FiLM åœ¨ Mamba ä¹‹å‰å¯¹å¸§çº§ç‰¹å¾è¿›è¡Œè°ƒåˆ¶
    # - DSPS åœ¨ Mamba å†…éƒ¨çš„ dt/B/C è·¯å¾„æ³¨å…¥æ¡ä»¶
    # åŒæ—¶å¯ç”¨ä¼šå¯¼è‡´å®éªŒç»“æœéš¾ä»¥è§£é‡Šï¼Œå› æ­¤å¼ºåˆ¶äº’æ–¥
    if config.model_config.use_dsps and config.model_config.use_moe_film:
        raise ValueError(
            "\n" + "=" * 70 + "\n"
            "âŒ é…ç½®é”™è¯¯ï¼šMoE-FiLM ä¸ DSPS ä¸èƒ½åŒæ—¶å¯ç”¨ï¼\n"
            "=" * 70 + "\n\n"
            "å½“å‰é…ç½®ï¼š\n"
            f"  use_moe_film = {config.model_config.use_moe_film}\n"
            f"  use_dsps = {config.model_config.use_dsps}\n\n"
            "è¿™ä¸¤ç§è°ƒåˆ¶æ–¹å¼æ˜¯äº’æ–¥çš„å®éªŒå¯¹ç…§ï¼š\n"
            "  â€¢ MoE-FiLMï¼šåœ¨ Mamba ä¹‹å‰å¯¹å¸§çº§ç‰¹å¾è¿›è¡Œè°ƒåˆ¶\n"
            "  â€¢ DSPSï¼šåœ¨ Mamba å†…éƒ¨çš„ dt/B/C è·¯å¾„æ³¨å…¥æ¡ä»¶\n\n"
            "è§£å†³æ–¹æ¡ˆï¼š\n"
            "  æ–¹æ¡ˆ1ï¼šä½¿ç”¨ MoE-FiLMï¼ˆé»˜è®¤ï¼‰\n"
            "         â†’ è®¾ç½® USE_DSPS=false (æˆ–ä¸è®¾ç½®)\n"
            "         â†’ è®¾ç½® NO_MOE_FILM=false (æˆ–ä¸è®¾ç½®)\n\n"
            "  æ–¹æ¡ˆ2ï¼šä½¿ç”¨ DSPS\n"
            "         â†’ è®¾ç½® USE_DSPS=true\n"
            "         â†’ è®¾ç½® NO_MOE_FILM=true\n\n"
            "  æ–¹æ¡ˆ3ï¼šä¸¤è€…éƒ½ä¸ç”¨ï¼ˆçº¯ Mambaï¼‰\n"
            "         â†’ è®¾ç½® USE_DSPS=false\n"
            "         â†’ è®¾ç½® NO_MOE_FILM=true\n"
            + "=" * 70
        )
    
    # è®­ç»ƒ
    trainer = Trainer(config)
    trainer.train()


if __name__ == '__main__':
    main()

